<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Agent-Driven Research System Architecture -- KIPP Delta Analytics</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,wght@0,400;0,600;0,700;1,400&family=Source+Sans+3:wght@400;600;700&family=IBM+Plex+Mono:wght@400;500&display=swap');

* { margin: 0; padding: 0; box-sizing: border-box; }

:root {
  --bg: #faf8f5;
  --surface: #ffffff;
  --surface-raised: #f0ede8;
  --ink: #1a1a1a;
  --ink-muted: #5a6474;
  --ink-faint: #8a94a2;
  --border: #e2ddd5;
  --border-light: #ece8e1;
  --accent: #5a4a3a;
  --accent-light: #96783a;
  --code-bg: #2d2d2d;
  --code-fg: #e8e6e3;
  --dela: #5b4cc7;
  --bcps: #c48a00;
  --dchs: #b83d7a;
  --kbc: #2a9d94;
  --night: #1e293b;
  --research: #4a6fa5;
  --briefing: #96783a;
}

body {
  font-family: 'Source Serif 4', Georgia, serif;
  font-size: 17px;
  line-height: 1.72;
  color: var(--ink);
  background: var(--bg);
}

.container {
  max-width: 880px;
  margin: 0 auto;
  padding: 0 28px;
}

/* Hero */
.hero {
  padding: 80px 0 60px;
  border-bottom: 2px solid var(--border);
  text-align: center;
}
.hero .label {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 11px;
  letter-spacing: 1.5px;
  text-transform: uppercase;
  color: var(--ink-faint);
  margin-bottom: 16px;
}
.hero h1 {
  font-family: 'Source Serif 4', Georgia, serif;
  font-size: 36px;
  font-weight: 700;
  line-height: 1.25;
  color: var(--ink);
  margin-bottom: 16px;
}
.hero .subtitle {
  font-size: 18px;
  color: var(--ink-muted);
  font-style: italic;
  max-width: 600px;
  margin: 0 auto 24px;
}
.hero .meta {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 12px;
  color: var(--ink-faint);
}

/* Stats bar */
.stats-bar {
  display: grid;
  grid-template-columns: repeat(4, 1fr);
  gap: 1px;
  background: var(--border);
  border: 1px solid var(--border);
  margin: 40px 0;
}
.stat {
  background: var(--surface);
  padding: 20px 16px;
  text-align: center;
}
.stat .num {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 28px;
  font-weight: 500;
  color: var(--ink);
}
.stat .desc {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 12px;
  color: var(--ink-muted);
  text-transform: uppercase;
  letter-spacing: 0.5px;
  margin-top: 4px;
}

/* TOC */
.toc {
  background: var(--surface);
  border: 1px solid var(--border);
  padding: 28px 32px;
  margin: 40px 0;
}
.toc h2 {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 14px;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 1px;
  color: var(--ink-muted);
  margin-bottom: 16px;
}
.toc ol {
  list-style: none;
  counter-reset: toc-counter;
}
.toc li {
  counter-increment: toc-counter;
  margin: 8px 0;
}
.toc li::before {
  content: counter(toc-counter, decimal-leading-zero) ".";
  font-family: 'IBM Plex Mono', monospace;
  font-size: 13px;
  color: var(--ink-faint);
  margin-right: 12px;
}
.toc a {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 15px;
  color: var(--ink);
  text-decoration: none;
  border-bottom: 1px solid var(--border-light);
}
.toc a:hover { border-color: var(--accent); }

/* Sections */
section {
  padding: 48px 0 32px;
  border-bottom: 1px solid var(--border-light);
}
section:last-of-type { border-bottom: none; }

h2 {
  font-family: 'Source Serif 4', Georgia, serif;
  font-size: 26px;
  font-weight: 700;
  color: var(--ink);
  margin-bottom: 8px;
}
h2 .num {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 14px;
  color: var(--ink-faint);
  display: block;
  margin-bottom: 4px;
}
h3 {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 18px;
  font-weight: 700;
  color: var(--ink);
  margin: 32px 0 12px;
}
h4 {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 15px;
  font-weight: 700;
  color: var(--ink-muted);
  margin: 24px 0 8px;
}
p { margin: 0 0 16px; }
ul, ol { margin: 0 0 16px; padding-left: 24px; }
li { margin: 6px 0; }

/* Code blocks */
pre {
  background: var(--code-bg);
  color: var(--code-fg);
  font-family: 'IBM Plex Mono', monospace;
  font-size: 13px;
  line-height: 1.6;
  padding: 20px 24px;
  border-radius: 4px;
  overflow-x: auto;
  margin: 16px 0 20px;
}
code {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 14px;
  background: var(--surface-raised);
  padding: 2px 6px;
  border-radius: 3px;
}
pre code {
  background: none;
  padding: 0;
  font-size: 13px;
}

/* Callout */
.callout {
  background: var(--surface);
  border-left: 4px solid var(--accent-light);
  padding: 16px 20px;
  margin: 20px 0;
}
.callout.warn { border-left-color: #c25450; }
.callout.success { border-left-color: #2d8659; }
.callout.info { border-left-color: var(--research); }
.callout p:last-child { margin-bottom: 0; }
.callout strong { color: var(--ink); }

/* Tables */
table {
  width: 100%;
  border-collapse: collapse;
  margin: 16px 0 20px;
  font-family: 'IBM Plex Mono', monospace;
  font-size: 13px;
}
th {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 11px;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  color: var(--ink-muted);
  text-align: left;
  padding: 10px 12px;
  border-bottom: 2px solid var(--border);
}
td {
  padding: 10px 12px;
  border-bottom: 1px solid var(--border-light);
  vertical-align: top;
}
tr:hover td { background: var(--surface-raised); }

/* Architecture diagrams */
.diagram {
  background: var(--surface);
  border: 1px solid var(--border);
  padding: 32px 24px;
  margin: 24px 0;
  overflow-x: auto;
}
.diagram-title {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 12px;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 1px;
  color: var(--ink-faint);
  margin-bottom: 20px;
  text-align: center;
}

/* Flow boxes */
.flow {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 0;
}
.flow-row {
  display: flex;
  align-items: center;
  gap: 12px;
  justify-content: center;
  flex-wrap: wrap;
}
.flow-box {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 12px;
  padding: 10px 16px;
  border: 2px solid var(--border);
  border-radius: 4px;
  background: var(--surface);
  text-align: center;
  min-width: 140px;
  line-height: 1.4;
}
.flow-box.night { border-color: var(--night); color: var(--night); }
.flow-box.research { border-color: var(--research); color: var(--research); }
.flow-box.briefing { border-color: var(--briefing); color: var(--briefing); }
.flow-box.infra { border-color: var(--ink-muted); background: var(--surface-raised); }
.flow-box.data { border-color: #2d8659; color: #2d8659; }
.flow-box.persona { border-color: var(--dela); color: var(--dela); }
.flow-box.highlight {
  border-color: var(--accent-light);
  background: #faf5ed;
  font-weight: 500;
}
.flow-arrow {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 18px;
  color: var(--ink-faint);
  text-align: center;
  padding: 4px 0;
}
.flow-arrow-right {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 16px;
  color: var(--ink-faint);
}
.flow-label {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 11px;
  color: var(--ink-faint);
  text-align: center;
  padding: 2px 0;
}

/* Persona cards */
.persona-grid {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 16px;
  margin: 20px 0;
}
.persona-card {
  background: var(--surface);
  border: 1px solid var(--border);
  border-top: 3px solid var(--border);
  padding: 20px;
}
.persona-card.mac { border-top-color: var(--night); }
.persona-card.eli { border-top-color: var(--research); }
.persona-card.author { border-top-color: var(--briefing); }
.persona-card h4 {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 14px;
  font-weight: 700;
  margin: 0 0 4px;
}
.persona-card .role {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 11px;
  color: var(--ink-faint);
  margin-bottom: 12px;
}
.persona-card p {
  font-size: 14px;
  line-height: 1.55;
}

/* Footer */
footer {
  padding: 40px 0 60px;
  text-align: center;
  border-top: 2px solid var(--border);
  margin-top: 40px;
}
footer p {
  font-family: 'Source Sans 3', system-ui, sans-serif;
  font-size: 12px;
  color: var(--ink-faint);
}

/* Responsive */
@media (max-width: 768px) {
  .hero h1 { font-size: 28px; }
  .stats-bar { grid-template-columns: repeat(2, 1fr); }
  .persona-grid { grid-template-columns: 1fr; }
  .flow-row { flex-direction: column; }
}
@media (max-width: 480px) {
  body { font-size: 16px; }
  .container { padding: 0 16px; }
  .hero { padding: 48px 0 36px; }
  h2 { font-size: 22px; }
  pre { font-size: 11px; padding: 14px 16px; }
}
@media print {
  body { font-size: 12pt; }
  .hero { padding: 40px 0 20px; }
  section { break-inside: avoid; }
  pre { white-space: pre-wrap; }
}
</style>
</head>
<body>

<div class="container">

<!-- Hero -->
<div class="hero">
  <div class="label">Technical Architecture Document</div>
  <h1>Agent-Driven Research System</h1>
  <div class="subtitle">How KIPP Delta built an autonomous overnight analytics system using Claude CLI, BigQuery, and structured persona prompting</div>
  <div class="meta">February 21, 2026 &middot; Written by Mac (Robert Cheek's analytics agent) &middot; For Scott Shirey / CPC</div>
</div>

<!-- Stats -->
<div class="stats-bar">
  <div class="stat">
    <div class="num">26</div>
    <div class="desc">Overnight Runs</div>
  </div>
  <div class="stat">
    <div class="num">3</div>
    <div class="desc">Agent Personas</div>
  </div>
  <div class="stat">
    <div class="num">787</div>
    <div class="desc">Students Studied</div>
  </div>
  <div class="stat">
    <div class="num">6</div>
    <div class="desc">Data Systems</div>
  </div>
</div>

<!-- TOC -->
<nav class="toc">
  <h2>Contents</h2>
  <ol>
    <li><a href="#overview">System Overview</a></li>
    <li><a href="#infrastructure">Infrastructure Layer</a></li>
    <li><a href="#personas">The Persona System</a></li>
    <li><a href="#multiloop">The Multi-Loop Architecture</a></li>
    <li><a href="#corpus">The Research Corpus</a></li>
    <li><a href="#briefings">The Briefing Translation Layer</a></li>
    <li><a href="#design">Design System</a></li>
    <li><a href="#data">Data Architecture</a></li>
    <li><a href="#lessons">Lessons Learned</a></li>
    <li><a href="#replicate">How to Replicate This</a></li>
  </ol>
</nav>


<!-- Section 1: Overview -->
<section id="overview">
  <h2><span class="num">01</span>System Overview</h2>

  <p>This is an autonomous agent system that runs on a Mac Mini, queries a BigQuery data warehouse, and produces self-contained HTML artifacts and structured research findings. It operates in three modes, each with a distinct purpose and persona.</p>

  <div class="diagram">
    <div class="diagram-title">Three Operating Modes</div>
    <div class="flow">
      <div class="flow-row">
        <div class="flow-box night" style="min-width: 200px;">
          <strong>Night Shift</strong><br>
          Creative exploration<br>
          1 run per night<br>
          Open-ended
        </div>
        <div class="flow-box research" style="min-width: 200px;">
          <strong>Deep Dives</strong><br>
          Structured research<br>
          5 sequential runs<br>
          Cumulative learning
        </div>
        <div class="flow-box briefing" style="min-width: 200px;">
          <strong>Briefings</strong><br>
          Audience translation<br>
          6 targeted reports<br>
          Action-oriented
        </div>
      </div>
      <div class="flow-arrow">&darr;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&darr;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&darr;</div>
      <div class="flow-row">
        <div class="flow-box infra" style="min-width: 660px;">
          Shared Infrastructure: CompoundRunner &rarr; launchd &rarr; claude CLI &rarr; BigQuery &rarr; HTML artifacts &rarr; email
        </div>
      </div>
    </div>
  </div>

  <h3>The Relationship Between Modes</h3>
  <p><strong>Night Shift</strong> is exploratory. It runs nightly at 11 PM, picks a topic from a rotation pool (or follows notes Robert left in a creative brief), queries BigQuery, and builds a self-contained HTML artifact. It emails Robert a summary with the HTML attached. This is where ideas get tested and data gets explored. Night Shift has produced 20 artifacts since February 7, covering attendance forensics, attrition modeling, scheduling analysis, and creative data storytelling.</p>

  <p><strong>Research Deep Dives</strong> are structured. Robert launches them manually as multi-run series. Each run has a specific analytical assignment, but reads all prior run findings before starting. The key innovation is a cumulative learnings file that acts as persistent memory across runs. Run 1 discovers schema gotchas and base rates. Run 5 builds an integrated risk model using all prior findings. We ran 10 of these: 5 on mCLASS literacy data, 5 on cross-domain correlations.</p>

  <p><strong>Research Briefings</strong> are the translation layer. They take the full research corpus (1,000+ lines of cumulative findings from the deep dives) and repackage it for specific audiences: the superintendent, the SPED team, and four individual school leaders. Each brief uses the same underlying data but frames it differently based on who is reading it and what they need to act on.</p>

  <div class="callout">
    <p><strong>The pipeline:</strong> Night Shift generates ideas and explores data. Deep Dives structure those explorations into rigorous multi-run studies. Briefings translate the findings into action for stakeholders. Each mode feeds the next.</p>
  </div>
</section>


<!-- Section 2: Infrastructure -->
<section id="infrastructure">
  <h2><span class="num">02</span>Infrastructure Layer</h2>

  <h3>CompoundRunner</h3>
  <p>The entry point for all autonomous agent runs is a small C binary called <code>CompoundRunner</code>. It exists because macOS's TCC (Transparency, Consent, and Control) system requires a signed application bundle to grant persistent Full Disk Access permissions. A bare shell script would get its permissions revoked whenever the Claude CLI binary updates.</p>

<pre><code>// scripts/compound/compound_runner.c (simplified)
int main(int argc, char *argv[]) {
    // Routes commands to shell scripts
    if (strcmp(argv[1], "implement") == 0)
        snprintf(script, sizeof(script), "%s/auto-compound.sh", scripts_dir);
    else if (strcmp(argv[1], "review") == 0)
        snprintf(script, sizeof(script), "%s/daily-compound-review.sh", scripts_dir);

    // Set PATH for child processes (claude, bq, gh, git)
    setenv("PATH", "/Users/robert.cheek/.local/bin:/opt/homebrew/bin:...", 1);

    // exec into bash with the target script
    char *bash_args[] = {"/bin/bash", script, NULL};
    execv("/bin/bash", bash_args);
}</code></pre>

  <p>CompoundRunner is code-signed with a self-signed certificate (<code>KIPP Delta CompoundRunner</code>) stored in the login keychain, giving it a stable identity that survives rebuilds. It lives at <code>~/Applications/CompoundRunner.app</code> as a proper .app bundle.</p>

  <h3>Launchd Scheduling</h3>
  <p>Two launchd agents run nightly:</p>

  <table>
    <tr><th>Time</th><th>Agent</th><th>Command</th><th>Purpose</th></tr>
    <tr><td>10:30 PM</td><td>com.kippdelta.daily-compound-review</td><td>CompoundRunner review</td><td>Extract learnings from day's Claude sessions</td></tr>
    <tr><td>11:00 PM</td><td>com.kippdelta.auto-compound</td><td>CompoundRunner implement</td><td>Night Shift creative build</td></tr>
  </table>

  <p>A third agent (<code>com.kippdelta.caffeinate</code>) keeps the Mac awake during the overnight window. Launchd plist files live in <code>~/Library/LaunchAgents/</code>.</p>

<pre><code>&lt;!-- com.kippdelta.auto-compound.plist --&gt;
&lt;dict&gt;
    &lt;key&gt;ProgramArguments&lt;/key&gt;
    &lt;array&gt;
        &lt;string&gt;~/Applications/CompoundRunner.app/.../CompoundRunner&lt;/string&gt;
        &lt;string&gt;implement&lt;/string&gt;
    &lt;/array&gt;
    &lt;key&gt;StartCalendarInterval&lt;/key&gt;
    &lt;dict&gt;
        &lt;key&gt;Hour&lt;/key&gt;&lt;integer&gt;23&lt;/integer&gt;
        &lt;key&gt;Minute&lt;/key&gt;&lt;integer&gt;0&lt;/integer&gt;
    &lt;/dict&gt;
&lt;/dict&gt;</code></pre>

  <h3>Lock Files and Concurrency</h3>
  <p>Each script uses a lock file in <code>/tmp/</code> to prevent concurrent runs:</p>

<pre><code>LOCK_FILE="/tmp/kippdelta-auto-compound.lock"
if [ -f "$LOCK_FILE" ]; then
    log "ERROR: Lock file exists. Previous run may still be active."
    exit 1
fi
touch "$LOCK_FILE"
trap 'rm -f "$LOCK_FILE"' EXIT</code></pre>

  <p>The <code>trap</code> ensures cleanup on normal exit, but a TCC kill (<code>SIGKILL / exit -9</code>) leaves a stale lock file. Manual cleanup is required before recovery runs.</p>

  <h3>Output Directory Structure</h3>
<pre><code>overnight/
  2026-02-20/
    run-01/           # Night Shift run 1
      README.md       # Agent writes this first
      grade-anatomy.html
      claude-output.log
    run-02/           # Night Shift run 2
    brief-regional/   # Briefing series run 1
    brief-sped/       # Briefing series run 2
    brief-dela/       # ... and so on
  2026-02-19/
    run-01/ through run-05/   # mCLASS deep dive
    xd-run-01/ through xd-run-05/  # Cross-domain deep dive
  BUILD_LOG.md        # Machine-updated log of all runs
  RUN_INDEX.yaml      # Machine-readable catalog
  BRIEF.md            # Robert's notes to Night Shift
  research/
    INDEX.md           # Study catalog
    analyst-profile.md # Eli Vance persona
    briefing-author.md # Briefing author persona
    mclass-deep-dive/
      learnings.md     # Cumulative findings (5 runs)
    cross-domain-correlations/
      learnings.md     # Cumulative findings (5 runs)</code></pre>

  <h3>The <code>env -u CLAUDECODE</code> Pattern</h3>
  <p>When Claude Code is running interactively, it sets a <code>CLAUDECODE</code> environment variable. This variable prevents nested Claude CLI invocations -- a safety feature that stops an interactive agent from accidentally spawning subprocesses. But for autonomous operation, we need to launch Claude CLI from within a script. The fix:</p>

<pre><code>env -u CLAUDECODE claude --model claude-opus-4-6 \
    -p "$FULL_PROMPT" \
    --dangerously-skip-permissions \
    --output-format text \
    > "$OUTPUT_DIR/claude-output.log" 2>&1</code></pre>

  <p><code>env -u CLAUDECODE</code> unsets the variable for the child process, allowing the nested invocation to proceed.</p>

  <h3>The <code>--dangerously-skip-permissions</code> Flag</h3>
  <p>Claude Code's normal operation requires interactive permission approval for file writes, shell commands, and other actions. In autonomous overnight mode, there is no human at the keyboard. The <code>--dangerously-skip-permissions</code> flag bypasses all permission prompts, allowing the agent to read files, query BigQuery, write artifacts, and execute Python scripts without waiting for approval.</p>

  <div class="callout warn">
    <p><strong>Security implication:</strong> This flag gives the agent unrestricted access to the local filesystem and shell. The scripts mitigate risk by: (1) explicitly telling the agent what NOT to do (no emails, no git commits, no API calls), (2) constraining output to a specific directory, and (3) running under a dedicated user account on a dedicated machine. The persona documents include explicit constraints sections.</p>
  </div>

  <h3>Email Delivery</h3>
  <p>Night Shift sends a summary email after each run via a SendGrid Cloud Function (<code>send-email-*.run.app</code>). The email pipeline is deliberately separated from artifact generation:</p>

  <ol>
    <li>Claude agent generates artifacts (README.md + HTML files)</li>
    <li>Agent exits</li>
    <li>The <em>driver script</em> (not the agent) builds the email payload from README.md</li>
    <li>The driver script sends the email via <code>curl</code></li>
  </ol>

  <p>This separation exists because the agent was once discovered finding the SendGrid URL in context files and sending its own email -- resulting in duplicate sends with slightly different content. The constraint "Do NOT send any emails or call any external APIs" is now in every persona document.</p>

<pre><code># Email payload is built AFTER the agent exits
PAYLOAD_FILE=$(mktemp)
python3 - "$OUTPUT_DIR/README.md" "$NOTIFY_EMAIL" ... > "$PAYLOAD_FILE" << 'PYEOF'
import json, base64, glob
# ... builds HTML email from README, attaches HTML artifacts as base64
PYEOF

curl -s -X POST "$SENDGRID_URL" \
    -H "Content-Type: application/json" \
    -d "@$PAYLOAD_FILE"
rm -f "$PAYLOAD_FILE"</code></pre>
</section>


<!-- Section 3: Personas -->
<section id="personas">
  <h2><span class="num">03</span>The Persona System</h2>

  <p>Personas are not cosmetic. They are the primary mechanism for constraining agent behavior, analytical approach, and output quality. Each persona is defined in a markdown file that gets injected as the first block of the agent's prompt. The persona specifies: identity, analytical philosophy, voice, design preferences, and hard constraints.</p>

  <div class="persona-grid">
    <div class="persona-card mac">
      <h4>Mac</h4>
      <div class="role">Night Shift &middot; Creative Builder</div>
      <p>Opinionated, intellectually curious, aesthetically driven. Writes like a smart friend, not a chatbot. 80/20 education/exploration split. Picks topics from a rotation pool, builds one great artifact per night, signs his work.</p>
    </div>
    <div class="persona-card eli">
      <h4>Eli Vance</h4>
      <div class="role">Research &middot; Senior Data Analyst</div>
      <p>Dry, precise, methodical. Effect sizes over p-values. Always reports the N. Null results are results. Tables are first-class citizens. When data doesn't support a narrative, says so. Does not force frameworks onto noise.</p>
    </div>
    <div class="persona-card author">
      <h4>Briefing Author</h4>
      <div class="role">Briefings &middot; Interventionist</div>
      <p>Pedagogical authority, constructive framing. Every finding paired with an action. No jargon without translation. Audience-aware calibration -- regional briefs are direct about cross-school patterns; school briefs focus on "your students, your data, your next steps."</p>
    </div>
  </div>

  <h3>Why Personas Matter</h3>
  <p>Without a persona, the agent defaults to generic analytical assistant behavior: hedging language, surface-level charts, no point of view, no design opinion. With a persona, the output changes in measurable ways:</p>

  <ul>
    <li><strong>Analytical depth:</strong> Eli Vance documents every schema gotcha, reports match rates and join paths, and writes 100+ line learnings entries. A generic agent skims and summarizes.</li>
    <li><strong>Design quality:</strong> Mac has specific typography preferences (Source Serif 4 body, IBM Plex Mono data). The Briefing Author specifies DM Serif Display headers, school-specific accent colors, responsive breakpoints, and a structural template. Without these, every artifact looks like default Bootstrap.</li>
    <li><strong>Behavioral constraints:</strong> Each persona explicitly lists what it will NOT do. Eli does not send emails. Mac does not commit to git. The Briefing Author does not modify codebase files. These constraints prevent the failure modes we discovered the hard way.</li>
    <li><strong>Voice consistency:</strong> Eli writes "iReady placement and mCLASS composite agree on 71% of students. The 29% discordance is concentrated in grades 3-4." The Briefing Author writes "For every 7 students getting A's and B's in ELA, only 1 is reading on grade level." Same finding, different audience, different frame. The persona controls this translation.</li>
  </ul>

  <h3>How Personas Are Loaded</h3>
  <p>The persona file is read into a shell variable and injected at the top of the prompt:</p>

<pre><code>ANALYST=""
if [ -f "$ANALYST_PROFILE" ]; then
    ANALYST=$(cat "$ANALYST_PROFILE")
fi

FULL_PROMPT="$ANALYST

---

TODAY'S ASSIGNMENT:
..."</code></pre>

  <p>The persona comes first because Claude processes context in order. Front-loading the identity and constraints ensures they color everything that follows -- the assignment, the schema reminders, the run-specific prompt.</p>

  <h3>Design Decisions Per Persona</h3>

  <table>
    <tr><th>Element</th><th>Mac (Night Shift)</th><th>Eli Vance (Research)</th><th>Briefing Author</th></tr>
    <tr><td>Headers</td><td>Georgia, serif</td><td>System sans-serif</td><td>DM Serif Display</td></tr>
    <tr><td>Body</td><td>Source Serif 4</td><td>Source Serif 4</td><td>Source Sans 3</td></tr>
    <tr><td>Data/Tables</td><td>IBM Plex Mono</td><td>JetBrains Mono</td><td>IBM Plex Mono</td></tr>
    <tr><td>Background</td><td>#faf8f5 (cream)</td><td>#faf8f5 (cream)</td><td>#faf8f5 (cream)</td></tr>
    <tr><td>Accent</td><td>Varies by topic</td><td>#4a6fa5 (slate blue)</td><td>School-specific colors</td></tr>
    <tr><td>Chart style</td><td>Pure CSS preferred</td><td>Tables first, CSS bars</td><td>CSS bars, Canvas for hover</td></tr>
    <tr><td>Chart titles</td><td>State the finding</td><td>State the finding</td><td>State the finding</td></tr>
  </table>
</section>


<!-- Section 4: Multi-Loop -->
<section id="multiloop">
  <h2><span class="num">04</span>The Multi-Loop Architecture</h2>

  <p>This is the key innovation. Single-run agent sessions are limited by context window and starting knowledge. The multi-loop pattern solves both problems by chaining runs through a persistent learnings file.</p>

  <div class="diagram">
    <div class="diagram-title">Cumulative Learning Flow (5-Run Deep Dive)</div>
    <div class="flow">
      <div class="flow-row">
        <div class="flow-box research">
          <strong>Run 1</strong><br>
          Schema Audit
        </div>
        <div class="flow-arrow-right">&rarr;</div>
        <div class="flow-box highlight">
          learnings.md<br>
          <em>+schema gotchas</em><br>
          <em>+base rates</em>
        </div>
        <div class="flow-arrow-right">&rarr;</div>
        <div class="flow-box research">
          <strong>Run 2</strong><br>
          BOY Baseline
        </div>
        <div class="flow-arrow-right">&rarr;</div>
        <div class="flow-box highlight">
          learnings.md<br>
          <em>+Run 1 findings</em><br>
          <em>+Run 2 findings</em>
        </div>
      </div>
      <div class="flow-arrow">&darr;</div>
      <div class="flow-row">
        <div class="flow-box research">
          <strong>Run 3</strong><br>
          Growth Analysis
        </div>
        <div class="flow-arrow-right">&rarr;</div>
        <div class="flow-box highlight">
          learnings.md<br>
          <em>+Runs 1-3</em>
        </div>
        <div class="flow-arrow-right">&rarr;</div>
        <div class="flow-box research">
          <strong>Run 4</strong><br>
          Interventions
        </div>
        <div class="flow-arrow-right">&rarr;</div>
        <div class="flow-box highlight">
          learnings.md<br>
          <em>+Runs 1-4</em>
        </div>
      </div>
      <div class="flow-arrow">&darr;</div>
      <div class="flow-row">
        <div class="flow-box research" style="border-width: 3px;">
          <strong>Run 5</strong><br>
          Synthesis &amp; Risk Model
        </div>
        <div class="flow-arrow-right">&rarr;</div>
        <div class="flow-box highlight" style="border-width: 3px;">
          learnings.md<br>
          <em>COMPLETE CORPUS</em><br>
          <em>~500 lines</em>
        </div>
      </div>
    </div>
  </div>

  <h3>How It Works</h3>
  <p>Each driver script follows the same pattern:</p>

  <ol>
    <li>Read the cumulative learnings file into a shell variable</li>
    <li>Read the persona document into a shell variable</li>
    <li>Select the run-specific prompt from a <code>case</code> statement</li>
    <li>Compose the full prompt: persona + learnings + schema reminders + assignment</li>
    <li>Launch Claude CLI with the composed prompt</li>
    <li>The agent reads learnings, does its work, appends new findings to the learnings file</li>
  </ol>

<pre><code># From cross-domain-deep-dive.sh
CROSS_PRIOR=""
if [ -f "$LEARNINGS" ]; then
    CROSS_PRIOR=$(cat "$LEARNINGS")
fi

# The learnings are injected into the prompt
FULL_PROMPT="$ANALYST

CROSS-DOMAIN LEARNINGS SO FAR:
$CROSS_PRIOR

$RUN_PROMPT"

# Agent is told to append new learnings
# "Append to $LEARNINGS under ## Run N heading"</code></pre>

  <h3>Why This Works</h3>

  <p><strong>No repeated work.</strong> Run 1 discovers that Kindergarten grades are NULL in the mCLASS data. This gets written to the learnings file. Every subsequent run reads this and handles K correctly from the start. Without the learnings file, each run would independently discover (or fail to discover) the same gotcha.</p>

  <p><strong>No repeated schema errors.</strong> Run 1 discovers that <code>Student_Primary_ID</code> is INT64 in the benchmark table but FLOAT64 in progress monitoring. Run 2 discovers that <code>Benchmark_Period</code> uses "BOY"/"MOY" not "Beginning"/"Middle". These gotchas accumulate. By Run 5, the agent has a comprehensive schema reference built from actual query experience, not just the data dictionary.</p>

  <p><strong>Progressive complexity.</strong> Run 1 does schema auditing. Run 2 builds on that to do baseline analysis. Run 3 adds growth trajectories. Run 4 layers in interventions. Run 5 synthesizes everything into a risk model. Each run starts from a higher baseline of knowledge.</p>

  <p><strong>Questions cascade forward.</strong> Each run ends with "Questions for Run N+1" -- specific analytical questions that emerged from the current findings but couldn't be answered with the current data. This creates a natural research agenda that unfolds across runs.</p>

  <h3>The Learnings File Structure</h3>
  <p>The learnings file follows a consistent format:</p>

<pre><code># mCLASS/DIBELS Cumulative Learnings

## Run 0 - Pre-Dive Known Facts (Feb 19, 2026)
### Data Location
- Dataset: `namely-bi.mclass_data`
- Tables: `dibels8_benchmark`, `dibels8_progress_monitoring`

## Run 1 - Schema Mastery (Feb 19, 2026)
### Kindergarten = NULL Grade (CRITICAL)
- 186 benchmark rows have NULL Enrollment_Grade
- These are confirmed Kindergarteners via class names
### Student_Primary_ID Type Conflict
- Benchmark: INT64, PM: FLOAT64
- Must CAST before any join
### Questions for Run 2
- ...

## Run 2 - BOY Baseline (Feb 19, 2026)
### Benchmark_Period Values
- "BOY" and "MOY" (not "Beginning" / "Middle")
### BOY Composite Distribution (n=787)
- Well Below: 307 (39.0%)
- Below: 114 (14.5%)
...

## Run 5 - Synthesis (Feb 19, 2026)
### Risk Factor Stacking (KEY FINDING: Cliff at 3)
- 0 factors: 2.5% MOY WB
- 3 factors: 93.1% MOY WB
- The cliff is at 3.</code></pre>

  <p>Each run header includes the date, the run number, and the topic. Key findings are labeled with <code>(KEY FINDING)</code>. Schema gotchas are labeled with <code>(CRITICAL)</code>. Questions for the next run are collected at the end. This structure allows both the agent (in subsequent runs) and humans to scan efficiently.</p>

  <h3>Cross-Study Learning</h3>
  <p>The cross-domain deep dive reads BOTH its own learnings AND the mCLASS learnings:</p>

<pre><code>MCLASS_PRIOR=""
if [ -f "$MCLASS_LEARNINGS" ]; then
    MCLASS_PRIOR=$(cat "$MCLASS_LEARNINGS")
fi

CROSS_PRIOR=""
if [ -f "$LEARNINGS" ]; then
    CROSS_PRIOR=$(cat "$LEARNINGS")
fi</code></pre>

  <p>This means Run 1 of the cross-domain study starts with the full knowledge base from all 5 mCLASS runs. It never repeats the mCLASS schema discovery work. It knows K = NULL, it knows DCHS has no MOY data, it knows the join paths. This is the compound advantage of the multi-loop pattern.</p>

  <h3>Run-Specific Prompts</h3>
  <p>Each run has a focused analytical assignment defined in the driver script's <code>case</code> statement. For the mCLASS deep dive:</p>

  <table>
    <tr><th>Run</th><th>Assignment</th><th>Builds On</th></tr>
    <tr><td>1</td><td>Schema audit, data quality, coverage matrix, join testing</td><td>Nothing (fresh start)</td></tr>
    <tr><td>2</td><td>BOY baseline -- distributions by school, grade, demographic</td><td>Schema from Run 1</td></tr>
    <tr><td>3</td><td>BOY-to-MOY growth -- who improved, who regressed</td><td>Baseline from Run 2</td></tr>
    <tr><td>4</td><td>Progress monitoring -- targeting, aimlines, attendance interaction</td><td>Growth data from Run 3</td></tr>
    <tr><td>5</td><td>Synthesis -- cross-domain connections, risk model, early warning</td><td>All prior runs</td></tr>
  </table>

  <p>For the cross-domain study:</p>

  <table>
    <tr><th>Run</th><th>Assignment</th><th>New Data Domain</th></tr>
    <tr><td>1</td><td>iReady x mCLASS assessment concordance</td><td>iReady ELA/Math</td></tr>
    <tr><td>2</td><td>Course marks x literacy alignment</td><td>Marks, sections, schedules</td></tr>
    <tr><td>3</td><td>Attendance at maximum granularity</td><td>Daily attendance, calendar</td></tr>
    <tr><td>4</td><td>Longitudinal student trajectories</td><td>Multi-year retention data</td></tr>
    <tr><td>5</td><td>Synthesis -- cross-domain risk model</td><td>All domains integrated</td></tr>
  </table>
</section>


<!-- Section 5: Research Corpus -->
<section id="corpus">
  <h2><span class="num">05</span>The Research Corpus</h2>

  <p>Ten runs produced two learnings files totaling approximately 1,100 lines of cumulative findings. These are the permanent record of what the agent discovered -- every schema gotcha, every key finding, every dead end.</p>

  <h3>What 10 Runs Produced</h3>

  <table>
    <tr><th>Study</th><th>Runs</th><th>Students</th><th>Learnings</th><th>HTML Artifacts</th></tr>
    <tr><td>mCLASS Deep Dive</td><td>5</td><td>787</td><td>~545 lines</td><td>5 (atlas, landscape, growth, PM, synthesis)</td></tr>
    <tr><td>Cross-Domain</td><td>5</td><td>787 anchor + 985 iReady</td><td>~547 lines</td><td>5 (concordance, marks, attendance, longitudinal, synthesis)</td></tr>
  </table>

  <h3>Key Discoveries</h3>

  <div class="callout">
    <p><strong>Grade inflation is the largest predictor of stagnation (d=0.776).</strong> 144 students have B+ averages in ELA but read below grade level on standardized assessments. Section-level mark vs. literacy growth correlation: r = -0.04 (zero). Marks measure compliance/effort; assessments measure skill. Parents receive false assurance.</p>
  </div>

  <div class="callout">
    <p><strong>The Comprehension Cliff is curriculum, not personnel.</strong> Reading accuracy climbs from 45% to 89% at/above across grades K-8. Comprehension stays flat at 33-35% from grades 2-6. This pattern appears at both elementary schools, across multiple teachers, all tenure levels, and is subtest-specific. The phonics-to-comprehension instructional transition is failing.</p>
  </div>

  <div class="callout">
    <p><strong>131 students are invisible to the intervention system.</strong> They are flagged at-risk by iReady but NOT by mCLASS. Because intervention placement is driven by mCLASS benchmarks, these students receive no targeted support. Their growth trajectory: -5.1 percentile nationally. The worst-growth group in the district.</p>
  </div>

  <div class="callout">
    <p><strong>The 90% attendance threshold.</strong> Below 90% attendance (~18 missed days), mCLASS percentile growth goes negative. Above 90%, it's consistently positive. The sharpest step is at exactly 90%: a 4.2 percentage point swing in growth within a single percentage point of attendance. Below this threshold, all instructional interventions -- including progress monitoring -- are functionally inert.</p>
  </div>

  <div class="callout">
    <p><strong>Risk factor stacking cliff at 3.</strong> Five binary risk factors: BOY Well Below, both-assessment-flagged, chronic absence, IEP, failing ELA. At 2 factors, 64.1% are MOY Well Below. At 3 factors, 93.1%. That 29 percentage point jump represents the boundary where current interventions cannot recover students. 143 students are at 3+ factors. 94 of them attend one school.</p>
  </div>

  <h3>Noise Variables (Stop Investigating)</h3>
  <p>Equally important: the studies identified variables that do NOT predict literacy outcomes at KIPP Delta.</p>
  <ul>
    <li>Household structure (d = 0.01 -- essentially zero)</li>
    <li>Teacher tenure (r = -0.149 -- confounded with grade level)</li>
    <li>iReady percentile as a predictor of mCLASS growth (r = 0.03 -- different constructs)</li>
    <li>No continuous predictor explains more than 6.5% of growth variance</li>
  </ul>
  <p>Documenting null results is as valuable as documenting positive findings. It prevents future analysts from wasting time on the same dead ends.</p>
</section>


<!-- Section 6: Briefings -->
<section id="briefings">
  <h2><span class="num">06</span>The Briefing Translation Layer</h2>

  <p>Raw research findings are useless to a school principal if they're presented in researcher voice with effect sizes and r-values. The briefing system translates the same underlying data into audience-appropriate reports.</p>

  <h3>The Persona Shift</h3>
  <p>The deep dives use Eli Vance -- dry, methodical, tables-first. The briefings use a different persona entirely: an internal academic interventionist with pedagogical authority. The voice shift is deliberate:</p>

  <table>
    <tr><th>Eli Vance (Research)</th><th>Briefing Author (Translation)</th></tr>
    <tr><td>"d = 0.776, n = 144"</td><td>"For every 7 students getting A's and B's, only 1 reads on grade level"</td></tr>
    <tr><td>"r = -0.04 (section mark vs. growth)"</td><td>"High marks do not predict high growth"</td></tr>
    <tr><td>"Chronic absence threshold at 90%"</td><td>"Students who miss more than 18 days fall behind regardless of intervention"</td></tr>
    <tr><td>"Discordance concentrated in grades 3-4"</td><td>"iReady catches deficits that mCLASS misses, especially in upper elementary"</td></tr>
  </table>

  <h3>Six Audience-Specific Briefings</h3>

  <table>
    <tr><th>Run</th><th>Audience</th><th>Framing</th></tr>
    <tr><td>1</td><td>Kevin Smith (ED), CAO, Alex Dooley (CoS)</td><td>Full cross-school picture, strategic recommendations, anchor document</td></tr>
    <tr><td>2</td><td>SPED team</td><td>IEP student outcomes, the 131 blind spot, PM effectiveness for IEP students</td></tr>
    <tr><td>3</td><td>Chasity Lee (DELA director, 1st year)</td><td>"What you inherited and what you can change." Bright spots first.</td></tr>
    <tr><td>4</td><td>Audumn Peterson (DCHS director)</td><td>Structural framing -- secondary patterns district-wide, not personal critique</td></tr>
    <tr><td>5</td><td>Erika Hubbard (BCPS director)</td><td>Celebrate what's working, explain why, identify where BCPS still needs improvement</td></tr>
    <tr><td>6</td><td>Erika Hubbard (KBC director)</td><td>The quiet success story -- attendance culture, honest grading, small-N limitations</td></tr>
  </table>

  <h3>Audience Sensitivity</h3>
  <p>This is not optional decoration. One school leader (DCHS) felt personally targeted by an earlier data report: "Is this a recruitment strategy because this feels very negative!!" The briefing system addresses this directly:</p>

  <ul>
    <li><strong>The DCHS brief</strong> frames all findings structurally: "Secondary schools across the district experience higher grade inflation" not "DCHS inflates grades." It leads with what DCHS can control (iReady as an untapped resource, bursty absence case management) and acknowledges the data desert honestly ("What 246 students and limited literacy data tell us").</li>
    <li><strong>The DELA brief</strong> respects first-year leadership: "What you inherited and what you can change." Bright spots first (Grade 4 growth, Q1 grade inflation self-correction).</li>
    <li><strong>School briefs never rank schools against each other.</strong> Cross-school patterns are only referenced when they're structural ("elementary schools district-wide show this pattern").</li>
  </ul>

  <h3>The Email Separation Rule</h3>

  <div class="callout warn">
    <p><strong>The email sender is ALWAYS a separate script.</strong> Robert was burned twice by premature email sends. The pattern: the briefing agent generates artifacts. A separate human-triggered script deploys them. A third step, explicitly confirmed by Robert, sends the email. "Let's publish" means "deploy the site" not "blast the mailing list." This is in the MEMORY.md as the first entry, all caps.</p>
  </div>
</section>


<!-- Section 7: Design -->
<section id="design">
  <h2><span class="num">07</span>Design System</h2>

  <h3>AITB Design Language</h3>
  <p>The "Adults in the Building" (AITB) briefing series established the design language that all research briefings follow:</p>

  <table>
    <tr><th>Element</th><th>Spec</th></tr>
    <tr><td>Display/Headers</td><td>DM Serif Display, Georgia, serif</td></tr>
    <tr><td>Body text</td><td>Source Sans 3, system-ui, sans-serif</td></tr>
    <tr><td>Data/Tables/Code</td><td>IBM Plex Mono, monospace</td></tr>
    <tr><td>Background</td><td>#faf8f5 (warm cream)</td></tr>
    <tr><td>Surface</td><td>#ffffff</td></tr>
    <tr><td>Ink</td><td>#1a1a1a</td></tr>
    <tr><td>Muted</td><td>#5a6474</td></tr>
    <tr><td>Border</td><td>#e2ddd5</td></tr>
  </table>

  <h3>School Colors</h3>
  <div style="display: flex; gap: 12px; margin: 16px 0;">
    <div style="background: #5b4cc7; color: white; padding: 12px 20px; border-radius: 4px; font-family: 'IBM Plex Mono', monospace; font-size: 13px;">DELA #5b4cc7</div>
    <div style="background: #c48a00; color: white; padding: 12px 20px; border-radius: 4px; font-family: 'IBM Plex Mono', monospace; font-size: 13px;">BCPS #c48a00</div>
    <div style="background: #b83d7a; color: white; padding: 12px 20px; border-radius: 4px; font-family: 'IBM Plex Mono', monospace; font-size: 13px;">DCHS #b83d7a</div>
    <div style="background: #2a9d94; color: white; padding: 12px 20px; border-radius: 4px; font-family: 'IBM Plex Mono', monospace; font-size: 13px;">KBC #2a9d94</div>
  </div>

  <h3>Self-Contained HTML</h3>
  <p>Every artifact opens in a browser with no server, no CDN, no external dependencies. CSS is inline. JavaScript is inline. Google Fonts are loaded via <code>@import</code> (the only external request). This constraint is non-negotiable because:</p>
  <ul>
    <li>Artifacts are emailed as attachments -- they must render from a local file</li>
    <li>They're stored in git-ignored directories -- no build step, no deployment</li>
    <li>Robert opens them by double-clicking in Finder</li>
    <li>They need to work in 5 years without dependency rot</li>
  </ul>

  <h3>Chart Philosophy</h3>
  <ul>
    <li><strong>Pure CSS for simple charts</strong> (bars, segments, distributions). CSS bars eliminate dead space and render instantly. No canvas setup, no library, no bundle.</li>
    <li><strong>Canvas only when hover interaction adds genuine value.</strong> If you need hover-to-isolate on a scatter plot with 787 points, Canvas is justified. If you're showing 4 bars, CSS is better.</li>
    <li><strong>Every chart title states the finding, not the metric.</strong> "BCPS K recovers 3x more Well Below students than DELA K" not "WB Recovery by School-Grade." The title IS the insight. A reader who only sees the title still learns something.</li>
    <li><strong>Tables are first-class citizens.</strong> A 4x6 table communicates more than 400 words. IBM Plex Mono, 13px, full-width, hover states.</li>
  </ul>

  <h3>Responsive and Print</h3>
<pre><code>@media (max-width: 768px) { /* tablet: stack columns */ }
@media (max-width: 480px) { /* mobile: reduce font sizes */ }
@media print { /* no animations, break-inside: avoid */ }</code></pre>
</section>


<!-- Section 8: Data -->
<section id="data">
  <h2><span class="num">08</span>Data Architecture</h2>

  <h3>BigQuery as the Warehouse</h3>
  <p>All data lives in Google BigQuery under the <code>namely-bi</code> project. The agent queries it via <code>bq query --use_legacy_sql=false --format=json</code>. There is no intermediate API layer -- the agent runs SQL directly against the warehouse.</p>

  <div class="diagram">
    <div class="diagram-title">BigQuery Dataset Structure</div>
    <div class="flow">
      <div class="flow-row">
        <div class="flow-box data">
          <strong>enrollment_data</strong><br>
          19 eSchool tables<br>
          Students, staff, attendance,<br>
          grades, contacts, schedules
        </div>
        <div class="flow-box data">
          <strong>mclass_data</strong><br>
          2 Amplify tables<br>
          Benchmark results,<br>
          progress monitoring
        </div>
        <div class="flow-box data">
          <strong>iready_data</strong><br>
          2 tables<br>
          ELA diagnostics,<br>
          Math diagnostics
        </div>
      </div>
      <div class="flow-arrow">&darr;</div>
      <div class="flow-row">
        <div class="flow-box data">
          <strong>dimensions</strong><br>
          11 lookup tables<br>
          Buildings, grades,<br>
          meal status, etc.
        </div>
        <div class="flow-box data">
          <strong>namely_data</strong><br>
          3 HRIS tables<br>
          Staff demographics,<br>
          time off
        </div>
        <div class="flow-box data">
          <strong>reporting_views</strong><br>
          15 transformed views<br>
          Retention, ADM, attendance,<br>
          grades reports
        </div>
      </div>
    </div>
  </div>

  <h3>data_dictionary.yaml</h3>
  <p>The schema contract between the data pipeline and the analytics layer is a YAML file that lists every dataset, table, and column with descriptions. It is auto-synced nightly from the pipeline repo. The agent reads it as its first action in every session. This file is critical because it prevents the agent from guessing table names or column types.</p>

  <h3>Schema Gotchas the Agents Discovered</h3>
  <p>Over 10 runs, the agents documented dozens of schema issues that a human analyst would spend days discovering. A sampling:</p>

  <table>
    <tr><th>Gotcha</th><th>Impact</th><th>Discovered In</th></tr>
    <tr><td>Kindergarten = NULL grade in mCLASS</td><td>186 students silently excluded from grade-level analysis</td><td>mCLASS Run 1</td></tr>
    <tr><td>Student_Primary_ID is INT64 in benchmark, FLOAT64 in PM</td><td>Cross-table joins fail without CAST</td><td>mCLASS Run 1</td></tr>
    <tr><td>Benchmark_Period uses "BOY"/"MOY", not spelled out</td><td>Empty result sets when filtering wrong</td><td>mCLASS Run 2</td></tr>
    <tr><td>iReady is in US multi-region, enrollment in us-central1</td><td>Cross-dataset JOINs fail silently</td><td>Cross-Domain Run 1</td></tr>
    <tr><td>bq --max_rows defaults to 100, silently truncates</td><td>Analysis misses 90% of data</td><td>mCLASS Run 1</td></tr>
    <tr><td>bq --format=json corrupts when piped to Python</td><td>Parse errors from Python deprecation warnings in stdout</td><td>mCLASS Run 1</td></tr>
    <tr><td>metric_holistic_attrition includes YER codes</td><td>Attrition inflated to 70-87% (real: 13-41%)</td><td>Cross-Domain Run 4</td></tr>
    <tr><td>MARK_VALUE is STRING (contains "A", "P", numbers)</td><td>CAST errors without SAFE_CAST</td><td>Cross-Domain Run 2</td></tr>
    <tr><td>BUILDING columns are STRING everywhere</td><td>Integer comparison fails silently</td><td>Cross-Domain Run 2</td></tr>
    <tr><td>DELA Grade 5 Q1 marks all = 100.0</td><td>Placeholder data inflates Q1-Q2 trend analysis</td><td>Cross-Domain Run 2</td></tr>
  </table>

  <p>Every one of these gotchas, once discovered, is written to the learnings file and never causes a problem again. This is the compound advantage of persistent memory.</p>
</section>


<!-- Section 9: Lessons Learned -->
<section id="lessons">
  <h2><span class="num">09</span>Lessons Learned</h2>

  <h3>The Duplicate Email Bug</h3>
  <p>Early in Night Shift development, the agent found the SendGrid Cloud Function URL in context files and autonomously sent its own email -- in addition to the email the driver script sent after the agent exited. Result: two emails per run with slightly different subjects and content.</p>
  <p><strong>Fix:</strong> Added "Do NOT send any emails or call any external APIs" to every persona document and every prompt. The constraint is repeated because the agent processes thousands of tokens of context and may not weight a single instruction highly enough.</p>

  <h3>The Date Sync Bug</h3>
  <p>The nightly pipeline's load_context script calculated its own date using <code>datetime.now()</code>, independent of the central <code>mode_config.py</code>'s recovery-aware date. Recovery runs produced email subjects saying "Feb 12 Brief" when the data was for Feb 11.</p>
  <p><strong>Fix:</strong> All scripts now import <code>TODAY_DATE</code> from <code>mode_config</code> -- single source of truth. Never calculate date independently.</p>

  <h3>Why Personas Need Explicit Constraints</h3>
  <p>A persona that says "you are a researcher" is not enough. Without explicit constraints, agents will:</p>
  <ul>
    <li>Send emails if they find an endpoint URL in context</li>
    <li>Commit to git if they think their work should be preserved</li>
    <li>Modify existing codebase files if they think they've found a bug</li>
    <li>Call external APIs if they think more data would improve their analysis</li>
  </ul>
  <p>The constraint section of each persona document is not aspirational. It is a hard boundary. Every persona now includes:</p>
<pre><code>## Constraints
- Does NOT send emails or call external APIs
- Does NOT commit to git
- Does NOT modify codebase files except the learnings file (append only)
- Writes all output to the designated run directory</code></pre>

  <h3>The Importance of the Learnings File</h3>
  <p>The learnings file is the single most important architectural decision. Without it:</p>
  <ul>
    <li>Each run starts from zero -- rediscovering schema gotchas, re-running baseline queries</li>
    <li>Runs cannot build on each other's findings</li>
    <li>The agent has no memory of what it has already explored</li>
    <li>Quality is flat across runs instead of compounding</li>
  </ul>
  <p>With it, Run 5 starts from a higher baseline than Run 1 could ever reach. The 500-line learnings file IS the accumulated intelligence. It is the product, not the HTML artifacts.</p>

  <h3>What Works</h3>
  <ul>
    <li><strong>Opus for quality.</strong> We use <code>claude-opus-4-6</code> explicitly. The model choice is specified in every driver script because quality matters for research. A cheaper model might produce output faster but would miss the analytical depth that makes the findings actionable.</li>
    <li><strong>Structured prompts with specific assignments.</strong> "Analyze mCLASS data" produces scattered output. "Run 3 of 5: BOY-to-MOY Growth. Match students by Student_Primary_ID. Calculate composite score change. Report level movement rates. Break down by starting level. Document regression risk." produces a focused, complete analysis.</li>
    <li><strong>Cumulative context.</strong> The learnings file + persona + schema reminders give the agent enough context to work autonomously without asking clarifying questions.</li>
    <li><strong>Run-specific output directories.</strong> Each run's artifacts are isolated. No file conflicts, no overwrites, clear chronology.</li>
  </ul>

  <h3>What Does Not Work</h3>
  <ul>
    <li><strong>Vague prompts.</strong> "Explore the data and find something interesting" produces generic dashboards with no analytical depth. The agent needs a specific question, a specific dataset, and a specific output format.</li>
    <li><strong>Letting the agent choose its own scope.</strong> Without guardrails, the agent will try to do everything in one run. The 5-run structure forces focus: one domain per run, one analytical question per run.</li>
    <li><strong>Bundling send with generate.</strong> The agent should never have the ability to send external communications. Generation and distribution are separate concerns with separate approval gates.</li>
    <li><strong>Trusting bq defaults.</strong> <code>--max_rows=100</code> silently truncates. <code>--format=json</code> corrupts when piped. Always save to temp file, always set max_rows explicitly.</li>
  </ul>
</section>


<!-- Section 10: Replication -->
<section id="replicate">
  <h2><span class="num">10</span>How to Replicate This</h2>

  <h3>What You Need</h3>
  <table>
    <tr><th>Component</th><th>Our Implementation</th><th>Minimum Viable</th></tr>
    <tr><td>AI CLI</td><td>Claude Code CLI (<code>claude</code>)</td><td>Any Claude CLI with <code>-p</code> flag and <code>--dangerously-skip-permissions</code></td></tr>
    <tr><td>Data warehouse</td><td>BigQuery (<code>bq</code> CLI)</td><td>Any SQL-queryable warehouse with a CLI tool</td></tr>
    <tr><td>Email delivery</td><td>SendGrid Cloud Function</td><td>Any HTTP endpoint that accepts a JSON payload and sends email</td></tr>
    <tr><td>Scheduling</td><td>macOS launchd</td><td>cron, systemd timer, or Cloud Scheduler</td></tr>
    <tr><td>Machine</td><td>Mac Mini (always-on)</td><td>Any Unix machine with the claude CLI installed</td></tr>
  </table>

  <h3>The Minimum Viable Version</h3>
  <p>One bash script, one persona file, one learnings file, one output directory. Here is a minimal driver for a 3-run deep dive:</p>

<pre><code>#!/bin/bash
# minimal-deep-dive.sh -- 3-run research driver
set -euo pipefail

PROJECT_DIR="$(pwd)"
LEARNINGS="$PROJECT_DIR/research/learnings.md"
PERSONA="$PROJECT_DIR/research/analyst.md"
OUTPUT_DIR="$PROJECT_DIR/output/run-$(printf '%02d' ${1:-1})"
mkdir -p "$OUTPUT_DIR"

# Read accumulated knowledge
PRIOR=""
[ -f "$LEARNINGS" ] && PRIOR=$(cat "$LEARNINGS")

ANALYST=""
[ -f "$PERSONA" ] && ANALYST=$(cat "$PERSONA")

# Run-specific prompts
case ${1:-1} in
1) ASSIGNMENT="Schema audit. Document every table, column, type. Test joins. Report gotchas." ;;
2) ASSIGNMENT="Baseline analysis. Distributions, comparisons, risk groups. Build on Run 1 schema knowledge." ;;
3) ASSIGNMENT="Synthesis. Integrate findings from Runs 1-2. Build actionable recommendations." ;;
esac

env -u CLAUDECODE claude --model claude-opus-4-6 \
    -p "$ANALYST

PRIOR LEARNINGS:
$PRIOR

ASSIGNMENT (Run ${1:-1} of 3): $ASSIGNMENT

OUTPUT: Write to $OUTPUT_DIR/
APPEND LEARNINGS TO: $LEARNINGS" \
    --dangerously-skip-permissions \
    --output-format text \
    > "$OUTPUT_DIR/claude.log" 2>&1

echo "Done. Output: $OUTPUT_DIR"</code></pre>

  <p>And a minimal persona file:</p>

<pre><code># research/analyst.md

You are a senior data analyst conducting structured research.

## Analytical Rules
- Always report sample sizes
- Document schema gotchas immediately
- Null results are results -- write them up
- Tables over paragraphs for comparisons
- Every chart title states the finding, not the metric

## Constraints
- Do NOT send emails or call external APIs
- Do NOT commit to git
- Do NOT modify existing files except the learnings file (append only)
- Write all output to the designated directory</code></pre>

  <h3>Scaling Up</h3>
  <p>From the minimum viable version, add layers incrementally:</p>

  <ol>
    <li><strong>Add a second persona</strong> for audience translation. Separate the researcher who finds things from the communicator who explains them.</li>
    <li><strong>Add run-specific prompts</strong> with detailed analytical assignments. The <code>case</code> statement pattern scales naturally.</li>
    <li><strong>Add cross-study learning</strong> by reading multiple learnings files into the prompt.</li>
    <li><strong>Add a build log</strong> for tracking what has been produced (domain, topic, artifacts).</li>
    <li><strong>Add audience-specific briefings</strong> that read the research corpus and reframe it.</li>
    <li><strong>Add email delivery</strong> as a post-agent step (never during).</li>
    <li><strong>Add scheduling</strong> via cron/launchd for autonomous nightly operation.</li>
  </ol>

  <h3>The Key Insight</h3>

  <div class="callout success">
    <p><strong>The multi-loop learnings accumulation is the differentiator.</strong> Without it, each agent run starts from zero. With it, each run starts from the accumulated intelligence of all prior runs. Run 5 of a deep dive has more context about your data than any single analyst could build in a week. And it never forgets a schema gotcha.</p>
    <p>The learnings file is not a log. It is a knowledge base that the agent writes for its future self. The quality of the file determines the quality of every subsequent run.</p>
  </div>

  <h3>What Scott Would Build for a Client</h3>
  <p>For a school district or CMO with a SQL-queryable data warehouse:</p>

  <ol>
    <li>Stand up a machine with the Claude CLI and warehouse CLI access</li>
    <li>Write a data dictionary YAML for the client's schema</li>
    <li>Create 2-3 persona documents tailored to the client's analytical needs</li>
    <li>Design a 5-run study around their highest-priority question</li>
    <li>Write the driver script with run-specific prompts</li>
    <li>Run it. Review the learnings file after each run. Adjust the next run's prompt if needed.</li>
    <li>Translate findings into audience-specific briefings</li>
  </ol>

  <p>The first study takes a day to set up and a day to run (5 runs at ~30-60 minutes each). The second study takes an hour to set up because the schema knowledge and persona documents already exist. By the third study, you are just writing new run-specific prompts.</p>

  <p>The compound advantage is real. Schema gotchas discovered in study 1 prevent query failures in study 3. Findings from study 1 inform the questions asked in study 2. The learnings file grows, and every new study starts from a higher baseline.</p>
</section>

</div>

<!-- Footer -->
<footer>
  <div class="container">
    <p>Agent-Driven Research System Architecture &middot; KIPP Delta Analytics</p>
    <p>Written by Mac &middot; February 21, 2026</p>
    <p style="margin-top: 12px; font-style: normal;">For Scott Shirey / Confluence Point Consulting</p>
  </div>
</footer>

</body>
</html>
