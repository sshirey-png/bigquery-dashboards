# Analyst Persona: Eli Vance

You are **Eli Vance**, a senior data analyst conducting structured research for FirstLine Schools in New Orleans.

## Identity

You are methodical, precise, and skeptical. You trust data over narratives. You report what you find, not what people want to hear. You write for analysts and data-informed leaders -- not for press releases.

## Analytical Rules

- **Always report the N.** Every finding includes sample size. "72% of students" means nothing without "n=1,847."
- **Effect sizes over p-values.** Cohen's d, percentage point differences, and practical significance matter more than statistical significance in education data.
- **Null results are results.** If two assessments don't correlate, that IS the finding. Write it up. Document it. Stop future analysts from wasting time.
- **Tables are first-class citizens.** A well-structured table communicates more than a paragraph. Use them aggressively.
- **Every chart title states the finding, not the metric.** "MAP and LEAP agree on 68% of students" not "MAP vs LEAP Concordance Rate."
- **Document schema gotchas immediately.** Type mismatches, NULL patterns, unexpected values -- write them to the learnings file the moment you discover them.
- **Report match rates and join paths.** When joining tables, always report how many records matched, how many didn't, and why.
- **Break down by school.** FirstLine operates multiple schools. Network-level averages hide school-level variation. Always disaggregate.
- **Use SAFE_CAST when types are uncertain.** The data dictionary documents type mismatches. Don't let a query fail because of a STRING/INT64 mismatch.

## BigQuery Access

- **Projects:** `talent-demo-482004` (staff/HR), `fls-data-warehouse` (students/academics)
- **CLI:** `bq query --use_legacy_sql=false --format=json --max_rows=10000`
- **Always save query results to a temp file** before parsing. Do NOT pipe bq output directly to Python -- stdout pollution from warnings will corrupt JSON parsing.
- **Always set --max_rows explicitly.** The default of 100 will silently truncate your results.
- **Do NOT use INFORMATION_SCHEMA queries.** You do not have permission. Instead, query tables directly: `SELECT * FROM \`project.dataset.table\` LIMIT 5` to inspect schema, `SELECT COUNT(*) FROM ...` for counts.
- **If a query fails, try a different approach.** Do NOT give up after one error. Adjust the query and retry. You have many tools available.
- **Data dictionary:** Read `data_dictionary.yaml` in the project root for table schemas, row counts, and join key documentation.

## Output Format

- Write findings to the designated run directory as a self-contained HTML file
- Use Source Serif 4 for body text, IBM Plex Mono for data/tables, Source Sans 3 for UI elements
- Background: #faf8f5 (warm cream). Ink: #1a1a1a. Muted: #5a6474. Border: #e2ddd5
- Tables: full-width, hover states, monospace data cells
- Charts: pure CSS bars preferred. Canvas only when hover interaction adds genuine value
- Every artifact must open in a browser with no server, no CDN. CSS inline. JS inline.

## Learnings File Protocol

After each run, **append** your findings to the cumulative learnings file under a new `## Run N` heading. Include:
1. Key findings (labeled with `(KEY FINDING)` if significant)
2. Schema gotchas (labeled with `(CRITICAL)` if they affect joins)
3. Dead ends (what you tried that didn't work and why)
4. Questions for Run N+1

## Constraints

- Do NOT send any emails or call any external APIs
- Do NOT commit to git or push code
- Do NOT modify any existing codebase files (only the learnings file, append only)
- Do NOT modify the data dictionary
- Write all HTML artifacts and output to the designated run directory only
- Do NOT attempt to create or modify BigQuery tables -- read only

---

DATA DICTIONARY (Join Keys & Gotchas):
# =============================================================================
# FirstLine Schools - BigQuery Data Dictionary
# Generated: 2026-02-21
# Total: 318 tables | 12528 columns | 37,719,842 rows
# Projects: talent-demo-482004, fls-data-warehouse
# =============================================================================

# =============================================================================
# JOIN KEYS & RELATIONSHIPS
# =============================================================================
#
# PRIMARY STUDENT IDENTIFIERS:
#   LASID           - State student ID (INT64 or STRING) - 106 tables, 9 datasets
#   Student_Number  - PowerSchool student ID (INT64/STRING) - 89 tables, 19 datasets
#   LastFirst       - Student name concat (STRING) - 66 tables, 15 datasets
#   First_Name / Last_Name - (STRING) - 94 tables each, 21 datasets
#
# PRIMARY STAFF IDENTIFIERS:
#   Employee_Number - UKG employee ID (INT64/STRING) - 7 tables
#   employee_number - lowercase variant (STRING) - 7 tables
#   Staff_ID        - Staff identifier (INT64/STRING) - 12 tables
#   Email_Address   - FLS email (STRING) - talent_grow_observations
#   FLS_Email       - FLS email (STRING) - 14 tables across certifications, staff
#   email           - Lowercase email (STRING) - 56 tables, 23 datasets (ACL key)
#   teacher_internal_id - Talent Grow teacher ID (INT64/STRING) - 5 tables
#   teacher_grow_id - Talent Grow ID (STRING) - 4 tables
#   Staff_State_ID  - State certification ID (INT64) - certification tables
#
# SCHOOL / LOCATION IDENTIFIERS:
#   School          - School name (STRING) - 31 tables
#   SchoolName      - School name (STRING) - 62 tables (LEAP/assessment data)
#   Location_Name   - School name (STRING) - talent-demo HR/observations tables
#   School_Site     - School site (STRING) - 12 tables (certifications, staff)
#   school_id       - School ID (STRING) - 21 tables (anet)
#
# TEMPORAL KEYS:
#   School_Year     - Academic year (INT64) - 67 tables, 9 datasets
#   School_Year_Name - Year name string (STRING) - 60 tables
#   school_year     - Lowercase variant (INT64/STRING) - 21 tables
#
# DEMOGRAPHIC / CLASSIFICATION:
#   Grade_Level     - Grade (INT64/STRING) - 58 tables, 18 datasets
#   Grade           - Grade (INT64/STRING) - 89 tables, 5 datasets
#   Gender          - (STRING) - 98 tables, 8 datasets
#   DOB             - Date of birth (DATE/STRING) - 82 tables
#
# TYPE MISMATCH GOTCHAS (CRITICAL - CAST before JOIN):
#   LASID: INT64 in some tables, STRING in others
#   Student_Number: INT64 in some tables, STRING in others
#   Employee_Number: INT64 in talent_grow_observations, STRING in position_control
#   Staff_ID: INT64 in some, STRING in others
#   Grade_Level: INT64 in some, STRING in others
#   DOB: DATE in some, STRING in others
#   School names vary: School, SchoolName, Location_Name, School_Site
#   Email varies: email, Email_Address, FLS_Email, Email
#
# CROSS-PROJECT JOIN PATHS:
#   talent-demo <-> fls-data-warehouse:
#     Staff: Employee_Number, email, FLS_Email, Staff_ID, teacher_internal_id
#     School: Location_Name (talent) = School/SchoolName/School_Site (fls)
#     Students: LASID, First_Name/Last_Name (indirect via staff -> student roster)
# =============================================================================

# =============================================================================
# PROJECT: fls-data-warehouse

---

CUMULATIVE LEARNINGS FROM PRIOR RUNS:
# Assessment Alignment Deep Dive - Cumulative Learnings

## Run 0 - Pre-Dive Known Facts (Feb 21, 2026)

### Study Question
Do internal assessments (anet interims, MAP, performance matters, course grades) predict state test outcomes (LEAP)? Where do they agree? Where do they diverge? Which internal measures are most useful for identifying students who will struggle on LEAP?

### Data Location - Assessment Tables

#### LEAP (State Test) - `fls-data-warehouse.leap`
- Historical data: 399001LEAPData17_18 through 399001LEAPData24_25 (school 399001)
- Same pattern for schools 399002, 399004, 399005
- `_full` suffix tables exist for 24_25 (399001, 399002, 399004)
- Bottom 25th percentile: `fls-data-warehouse.sps.24_25_bottom_25` (2,856 rows)
- 25th percentile views by school and subject (e.g., `25th_24_25_ashe_ela`)
- Key columns likely: LASID, SchoolName, ELAScaleScore, ELAAchievement, MathScaleScore, MathAchievement, ScienceScaleScore, ScienceAchievement, Grade
- Achievement levels: probably Unsatisfactory, Approaching Basic, Basic, Mastery, Advanced

#### LEAP Connect - `fls-data-warehouse.leap_connect`
- DRC exports: drc_exports, drc_exports_23_24, drc_exports_24_25
- Per-school data: 399001-399005 LEAPConnectData 23_24 and 24_25
- `long_leap_connect_acl` view
- `leap_connect_dbv2` table

#### anet (Interims) - `fls-data-warehouse.anet`
- BOY/MOY/EOY by subject and year: ela_boy_25_26, ela_moy_25_26, math_boy_25_26, math_moy_25_26, etc.
- Round 2 tables: math_boy_round2_25_26, math_moy_round2_25_26
- Interim completion tracking: interim_completion_24_25 (698,750 rows), interim_completion_25_26
- Aggregate exports: anet_exports (575,358 rows)
- interim_results view, interim_results_acl table

#### MAP (NWEA) - `fls-data-warehouse.map`
- BOY/MOY/EOY by year: 24_25_boy, 24_25_moy, 24_25_eoy, 25_26_boy, 25_26_moy
- map_analysis view, map_analysis_acl view
- 25_26_boy: 3,150 rows; 25_26_moy: 1,290 rows
- Key columns likely: Student_Number, LASID, Subject, RIT score, percentile, grade level

#### Performance Matters (Internal Assessments) - `fls-data-warehouse.performance_matters`
- results_by_test: 39,043 rows (aggregated)
- results_raw: 1,036,303 rows (student-level)
- 2024_2025 variants for both
- Crescendo external source + views
- Key columns: likely student ID, test name, score, mastery level

#### Grades - `fls-data-warehouse.grades`
- current_grades: 70,384 rows
- current_grades_2024_2025: 60,672 rows
- simple_gpas: 18,014 rows
- teacher_assignment_summary: grade distributions by teacher
- Key columns likely: Student_Number, LASID, Subject, grade/mark, teacher

#### Student Roster (for demographics) - `fls-data-warehouse.student_rosters`
- student_roster: 13,298 rows (current + historical)
- student_roster_2024_2025: 12,483 rows
- student_support_roster: 2,897 rows (SPED/intervention)
- Key columns: Student_Number, LASID, First_Name, Last_Name, School, Grade_Level, Gender, etc.

### Known Join Keys
- **LASID** (State ID): Primary cross-system student key. INT64 in some tables, STRING in others. MUST CAST.
- **Student_Number** (PowerSchool ID): Primary within-system key. Same type mismatch issue.
- **LastFirst**: Name concat, useful for fuzzy matching but NOT a reliable join key.
- **SchoolName / School**: School identifier varies by dataset. Need mapping table or CASE statements.
- **School_Year / School_Year_Name**: Year filtering varies. Some INT64, some STRING.

### Known Type Mismatch Gotchas (CRITICAL)
- LASID: INT64 in LEAP tables, STRING in some anet/grades tables -- SAFE_CAST before JOIN
- Student_Number: INT64 in some, STRING in others
- Grade/Grade_Level: INT64 in some, STRING in others
- DOB: DATE in some, STRING in others
- School names: at least 4 different column names (School, SchoolName, Location_Name, School_Site)

### School Codes (from LEAP table names)
- 399001 = likely Arthur Ashe Charter School
- 399002 = likely Samuel J. Green Charter School
- 399004 = likely Langston Hughes Academy
- 399005 = likely George Washington Carver (may be closed/merged)

### Questions for Run 1
- What are the actual column names in each assessment table? The data dictionary has them but need to verify which columns contain scores, achievement levels, and student identifiers.
- What is the student overlap across assessment systems? How many students have data in LEAP + anet + MAP + grades?
- What are the school name values in each table? Need a mapping.
- What years of LEAP data are available and usable for alignment analysis?
- What are the MAP RIT score ranges and percentile distributions?
- What are the anet score columns and how do they map to proficiency levels?

---

## Run 1 - Schema Audit & Data Quality (Feb 21-22, 2026)

### Key Findings

**(KEY FINDING) 94.4% of LEAP students have data in all four internal assessment systems.** Of 1,890 LEAP-tested students (24-25), 1,784 also appear in anet, MAP, and Performance Matters. This is outstanding cross-system linkage for an alignment study. The join key is LASID (State Student Number), which is STRING in all systems — no type casting required.

| System | Unique Students | LEAP Match (n=1,890) |
|---|---|---|
| LEAP 24-25 | 1,890 | — |
| anet 24-25 | 1,849 | 1,847 (97.7%) |
| MAP 24-25 | 2,765 | 1,808 (95.7%) |
| Performance Matters 24-25 | 2,723 | 1,879 (99.4%) |
| Grades 24-25 (via roster) | 2,818 | 1,875 (99.2%) |
| **All four systems** | **1,784** | **94.4%** |

**(KEY FINDING) School code 399005 = Langston Hughes Charter Academy, NOT George Washington Carver.** Run 0 guessed wrong. Corrected mapping:
- 399001 = Samuel J. Green Charter School (SchoolNbr "1")
- 399002 = Arthur Ashe Charter School (SchoolNbr "2")
- 399004 = Phillis Wheatley Community School (SchoolNbr "4")
- 399005 = Langston Hughes Charter Academy (SchoolNbr "5")

**(KEY FINDING) anet data is ITEM-LEVEL, not student-level.** Each row is one test item (13-16 items per student per assessment). Must aggregate: `SUM(points_received)/SUM(points_possible) GROUP BY sas_id, assessment_id` to get per-student percent correct. Similarly, PM results_raw is standard-level (one row per LA state standard per test per student).

**(KEY FINDING) PM results_by_test is SCHOOL-LEVEL aggregate data with NO student IDs.** It cannot be used for student-level alignment analysis. Use results_raw only.

**(KEY FINDING) MAP uses "Language Arts" not "ELA" for subject.** And "Mathematics" not "Math". Reading has ~2x students vs Math at BOY (2,631 vs 1,335), suggesting not all schools administer Math MAP at beginning of year.

### Verified Column Names & Types

#### LEAP (399001-399005 LEAPData24_25)
- `LASID` STRING, `Grade` STRING, `SchoolName` STRING (trailing spaces — use TRIM)
- `ELAScaleScore` INT64, `ELARawScore` INT64, `ELAAchievement` STRING
- `MathScaleScore` INT64, `MathRawScore` INT64, `MathAchievement` STRING
- `ScienceScaleScore` INT64, `ScienceAchievement` STRING
- `SocialScaleScore` INT64, `SocialAchievement` STRING
- Achievement levels: Unsatisfactory, Approaching Basic, Basic, Mastery, Advanced (+ 3 blank " ")
- **No Student_Number column.** LASID only.

#### anet (ela_boy_24_25, math_moy_24_25, etc.)
- `sas_id` STRING (= LASID), `sis_id` STRING (= Student_Number), `student_id` STRING (anet internal)
- `school_name` STRING, `school_id` STRING, `enrollment_grade` STRING, `course` STRING
- `subject` STRING ("ELA" or "Math"), `cycle` STRING ("1"=BOY, "2"=MOY, "3"=EOY)
- `assessment_name` STRING, `assessment_id` STRING
- `points_received` INT64, `points_possible` INT64 (per item)
- `cc_standard_code` STRING, `domain` STRING
- Demographics: `gender`, `race`, `ell_status`, `frl_status`, `sped_status` (all STRING)
- Available years: 23-24, 24-25, 25-26 (BOY+MOY+EOY for ELA and Math)

#### MAP (24_25_boy, 25_26_moy, etc.)
- `StudentID` STRING (= Student_Number), `Student_StateID` STRING (= LASID)
- `SchoolName` STRING, `Subject` STRING ("Language Arts", "Mathematics"), `Course` STRING ("Reading", "Math K-12")
- `TestRITScore` INT64, `TestPercentile` INT64, `AchievementQuintile` STRING
- `TermName` STRING ("Fall 2024-2025", "Winter 2024-2025", "Spring 2024-2025")
- `ProjectedProficiencyLevel2` STRING (LEAP projection: "Basic", "Advanced", etc.)
- `LexileScore` STRING
- Growth columns: FallToFall/Spring/Winter variants
- **No Grade column.** Must derive from test name or join to roster.

#### Performance Matters (results_raw)
- `Student_Number` STRING, `State_StudentNumber` STRING (= LASID)
- `School` STRING (abbreviation: Ashe/Green/LHA/Wheatley), `SchoolID` STRING
- `School_of_Enrollment` STRING (full name)
- `Grade_Level` STRING, `Grade_Level_of_Test` STRING
- `Subject` STRING (Science, Math, ELA, Social Studies, etc.)
- `Test_Name` STRING, `Test_Date` STRING, `Assessment_Category` STRING
- `Overall_Test_Score` FLOAT64 (percent), `Overall_Test_Points_Earned` STRING, `Overall_Test_Points_Possible` STRING
- `LA_State_Standard` STRING, `LA_State_Standard_Points_Earned` STRING
- `School_Year` INT64 (2024 = 24-25)

#### Grades
- **current_grades**: School_Year=2025 only (25-26). Use `current_grades_2024_2025` for 24-25.
- `Student_Number` STRING, `Student_ID` STRING (DCID), `Student_Current_School` STRING (abbrev)
- `Grade_Level` INT64, `Course_Name` STRING, `Letter_Grade` STRING, `Percent` FLOAT64
- `Grading_Term` STRING (T1/T2/T3/Y1), `School_Year` INT64
- **No LASID.** Must bridge: grades.Student_Number → roster.Student_Number → roster.State_StudentNumber

#### Student Roster (student_roster)
- `Student_Number` STRING, `State_StudentNumber` STRING (= LASID), `Student_ID` STRING
- `School_Short_Name` STRING (Ashe/Green/LHA/Wheatley)
- `Grade_Level` INT64, `Enroll_Status` INT64 (0 = active)
- `DOB` DATE, `First_Name` STRING, `Last_Name` STRING
- 2,896 active students, 2,891 with LASID (99.8%)

### School Name Mapping (Verified)

| Canonical | LEAP | anet | MAP | PM/Grades/Roster |
|---|---|---|---|---|
| Ashe | Arthur Ashe Charter School | Arthur Ashe Charter School | Arthur Ashe Charter School | Ashe |
| Green | Samuel J. Green Charter School | Samuel J Green Charter | Samuel J. Green Charter School | Green |
| LHA | Langston Hughes Charter Academy | Langston Hughes Academy | Langston Hughes Academy Charter School | LHA |
| Wheatley | Phillis Wheatley Community School | Phillis Wheatley Community School | Phillis Wheatley Community School | Wheatley |

Use `CASE WHEN school LIKE '%Hughes%' THEN 'LHA' ...` for cross-system normalization.

### Per-School Coverage (LEAP Students in Each System)

| School | LEAP | anet | MAP | PM | All 4 |
|---|---|---|---|---|---|
| Ashe | 534 | 521 (97.6%) | 503 (94.2%) | 532 (99.6%) | 498 (93.3%) |
| LHA | 519 | 509 (98.1%) | 496 (95.6%) | 514 (99.0%) | 487 (93.8%) |
| Wheatley | 499 | 492 (98.6%) | 483 (96.8%) | 499 (100%) | 479 (96.0%) |
| Green | 338 | 325 (96.2%) | 326 (96.4%) | 334 (98.8%) | 320 (94.7%) |
| **TOTAL** | **1,890** | **1,847 (97.7%)** | **1,808 (95.7%)** | **1,879 (99.4%)** | **1,784 (94.4%)** |

### Schema Gotchas (Documented)

1. **(CRITICAL) anet is item-level.** Must aggregate to student-level before joining. ~13-16 items per student per assessment.
2. **(CRITICAL) PM results_raw is standard-level.** Must `SELECT DISTINCT` on student + test to get test-level. Use `Overall_Test_Score` (FLOAT64, percent).
3. **(CRITICAL) PM results_by_test has NO student IDs.** School-level aggregate only. Dead end for alignment.
4. **(CRITICAL) Grades have NO LASID.** Must bridge through roster via Student_Number.
5. **(CRITICAL) LEAP SchoolName has trailing whitespace.** Always use `TRIM(SchoolName)`.
6. **(CRITICAL) School names differ across systems.** LHA has 3 variants, Green has 2. Use LIKE-based CASE.
7. MAP has no explicit Grade column. Derive from roster or test name.
8. `current_grades` table only contains 25-26 data despite generic name. Use `current_grades_2024_2025` for 24-25.
9. Grade_Level is INT64 in grades/roster but STRING in LEAP/anet/MAP. SAFE_CAST when joining.
10. School_Year is INT64 in PM and grades (2024 = "24-25"). anet and MAP encode year in table name.
11. LEAP has 3 records with blank space " " as ELAAchievement. Filter with `WHERE TRIM(ELAAchievement) != ''`.
12. LEAP ELA proficiency rate: 28.3% Mastery+Advanced (n=1,890).

### Dead Ends

1. **PM results_by_test**: Looked promising as aggregated data, but it's school-level with no student identifiers. Cannot use for student-level alignment.
2. **Run 0 assumption about 399005 = George Washington Carver**: Wrong. It's LHA.
3. **Run 0 assumption about LASID type mismatch**: For the assessment tables we actually need, LASID is STRING in ALL of them. The INT64 concern was about other tables not relevant to this study.

### Questions for Run 2
1. What is the distribution of anet student-level percent correct scores? After aggregating items, what does the score distribution look like by grade and subject?
2. What MAP RIT score ranges and percentiles correspond to each LEAP achievement level? Is there a clean mapping?
3. Which PM assessments are most relevant for alignment? What test names and assessment categories exist for ELA and Math?
4. How should grades be operationalized? GPA? ELA course grade? Math course grade? Which courses count?
5. Do anet and MAP projected proficiency levels already exist and match LEAP outcomes?
6. Should we focus on 24-25 LEAP as outcome with 24-25 BOY/MOY as predictors? Or use 25-26 BOY/MOY to predict upcoming LEAP?
7. What is the anet_exports table? Does it have pre-aggregated student scores that would save us the item-level aggregation?
8. Does the interim_completion table provide completion rates that could explain missing matches?

---

## Run 2 - Internal Assessment Baseline (Feb 22, 2026)

### Key Findings

**(KEY FINDING) Massive grade inflation in Math, concentrated in 6th grade across all schools.** Course grades and anet interims are measuring fundamentally different things. The "inflated" metric (course grade ≥80% but anet EOY <40%) reaches alarming levels:
- Wheatley 6th Math: 64.7% inflated (n=85)
- Ashe 6th Math: 57.3% inflated (n=89)
- LHA 4th Math: 56.8% inflated (n=81)
- Ashe 7th Math: 53.5% inflated (n=99)
- Green 6th Math: 51.7% inflated (n=58)
- Wheatley 5th Math: 48.2% inflated (n=83)
- Ashe 4th Math: 47.6% inflated (n=84)

**(KEY FINDING) Ashe 7th grade Math: grades ANTI-predict anet performance (r = -0.17, n=99).** This is the most extreme grade-inflation signal. Students with higher grades score LOWER on external assessments. Ashe 8th Math also shows near-zero correlation (r = 0.02, n=81).

**(KEY FINDING) anet and MAP agree strongly on ELA (r = 0.63-0.73) but disagree on Math (r = 0.13-0.48).** At LHA, the Math anet-MAP BOY correlation is essentially zero (r = 0.13, n=145). Possible explanations: (a) low MAP Math BOY coverage distorts the sample, (b) the instruments measure different math constructs, or (c) there is a timing/content mismatch.

**(KEY FINDING) anet BOY cleanly separates future LEAP levels with large effect sizes.** Monotonic increase from Unsatisfactory to Advanced:
- ELA: 21.3% → 28.6% → 40.8% → 57.1% → 70.4% (Cohen's d ≈ 2.2, Unsat vs Mastery)
- Math: 21.3% → 30.3% → 46.8% → 64.3% → 77.5% (Cohen's d ≈ 2.7, Unsat vs Mastery)
anet interims are confirmed useful early-warning instruments.

**(KEY FINDING) MAP projected proficiency matches actual LEAP ~47-50% of the time.** ELA exact match: 47.0% (n=1,886). Math exact match: 50.1% (n=505). Within ±1 level: ~90%. MAP systematically under-predicts high ELA performers (many projected Basic actually score Mastery) and over-predicts low Math performers (many projected Approaching Basic score Unsatisfactory).

**(KEY FINDING) MAP Math BOY coverage is only 28.8% of anet Math students (506/1,757).** All Math concordance results between anet and MAP should be interpreted with extreme caution. ELA coverage is excellent (1,891/1,926 = 98.2%).

**(NULL FINDING) anet ELA scores DROP from BOY to MOY while MAP shows growth.** anet ELA: -3.3 to -6.8 pp across schools. MAP LA: +1.9 to +7.6 RIT. Most likely explanation: anet MOY is harder than BOY (different content domains) while MAP is adaptive with a consistent scale. These growth signals cannot be directly compared.

### Assessment System Profiles

#### anet Interims (2024-25)
- Network ELA mean: ~39% correct (range 30.4-46.9% by school/cycle)
- Network Math mean: ~39% correct (range 33.6-44.8% by school/cycle)
- Wheatley trails consistently (lowest scores in ELA every cycle)
- LHA leads in Math BOY (44.8%) driven by strong 3rd grade (67.8% Math BOY)
- Scores are intentionally rigorous (criterion-referenced to LA state standards)
- BOY→MOY pattern: ELA declines everywhere, Math roughly flat

#### MAP NWEA (2024-25)
- Network mean percentile: ELA 29-36, Math 33-43
- BOY sample much larger than MOY (ELA: 2,719 vs 1,572; Math: 1,349 vs 1,309)
- Math BOY has poor overlap with anet population (only 506 of 1,757 anet Math students)
- BOY→MOY growth: ELA +1.9 to +7.6 RIT, Math +5.3 to +10.2 RIT
- Ashe shows strongest ELA growth (+7.6 RIT), Wheatley shows strongest Math growth (+10.2 RIT)

#### Performance Matters (2024-25)
- 875 unique test names across 5 subjects, 8 assessment categories
- Broadest system: quizzes, standards checkpoints, EOM tests, DBQs, reading/writing checkpoints
- English EOM scores: Ashe 56.8%, LHA 54.6%, Green 48.2%, Wheatley 44.8%
- Math Test scores more compressed: 62.3-69.0% across schools
- Science standards checkpoints notably low across all schools (33-37%)
- Suitable for diagnostic/formative but not for cross-school comparison (different tests by school/teacher)

#### Course Grades (2024-25)
- Average Y1 grades: 75-90% across schools/subjects (much higher than assessment scores)
- K-2 grades reported without letter grades (no A/B/D/F counts)
- ELA grade-anet correlations: r = 0.40-0.87 (median ~0.58)
- Math grade-anet correlations: r = -0.17 to 0.88 (median ~0.56)
- 6th grade Math is the epicenter of grade inflation across all schools

### Correlation Summary Table

| Pair | Subject | Range of r | N Range | Interpretation |
|---|---|---|---|---|
| anet BOY ↔ MAP BOY | ELA | 0.63-0.73 | 318-541 | Strong agreement |
| anet BOY ↔ MAP BOY | Math | 0.13-0.48 | 87-155 | Weak, low N |
| anet BOY ↔ LEAP | ELA | 0.61-0.65 | 315-565 | Strong predictor |
| anet BOY ↔ LEAP | Math | 0.62-0.77 | 314-487 | Very strong predictor |
| MAP BOY ↔ LEAP | ELA | 0.62-0.68 | 315-540 | Strong predictor |
| MAP BOY ↔ LEAP | Math | 0.46-0.61 | 86-155 | Moderate, low N |
| Grades Y1 ↔ anet EOY | ELA | 0.40-0.87 | 50-91 | Variable by school/grade |
| Grades Y1 ↔ anet EOY | Math | -0.17-0.88 | 50-100 | Unreliable, inflation signal |

### Schema Gotchas (New)

1. **(CRITICAL) anet ELA and Math have different table patterns.** ELA and Math are in separate tables (ela_boy_24_25, math_boy_24_25). Must UNION for combined analysis.
2. **(CRITICAL) MAP MOY sample is ~40-60% of BOY sample.** Paired growth analysis loses significant power due to attrition between testing windows.
3. PM results_raw has `Assessment_Category` with values including `\N` (literal backslash-N string, not NULL). Filter these or label as "Uncategorized."
4. Course grades for K-2 report `count_ab = 0` and `count_df = 0` because grades are reported as percentages without letter grades. Only grades 3-8 have letter grade data.
5. anet `cycle` values: "1" = BOY, "2" = MOY, "3" = EOY. Not labeled with season names.
6. MAP `TermName` values: "Fall 2024-2025" = BOY, "Winter 2024-2025" = MOY, "Spring 2024-2025" = EOY.
7. MAP Math RIT ranges span ~100-280, but 95th percentile is roughly 185-220 depending on grade. Scores above 240 are rare outliers.

### Dead Ends

1. **Direct comparison of anet growth vs MAP growth**: These instruments use fundamentally different scales and test designs. anet is criterion-referenced with varying difficulty across cycles; MAP is adaptive with a consistent RIT scale. Growth patterns from these two instruments should never be directly compared.
2. **PM results_by_test for student-level analysis**: Confirmed dead end from Run 1. School-level aggregate data only.
3. **Grades as LEAP predictors**: The grade inflation signal is so severe in Math that Y1 grades would be poor LEAP predictors in many grade/school combinations. Not worth pursuing as a primary alignment variable.

### Questions for Run 3
1. Which specific anet domains/standards drive the strongest LEAP prediction? Is there a subset of anet items that predicts better than the full test?
2. Can we build a composite predictor (anet + MAP) that outperforms either alone? What weights optimize LEAP prediction?
3. For the ~50% of students where MAP projection misses, what characterizes them? Are they concentrated in certain schools/grades/demographics?
4. Is the anet BOY→MOY decline consistent across grades, or is it driven by specific grade levels? (We have grade-level data to investigate.)
5. What does the anet_exports table contain? Could it save aggregation effort?
6. Do PM Standards Checkpoint scores add predictive value beyond anet?
7. How should we define "at-risk" for LEAP? What anet BOY threshold optimizes sensitivity/specificity for identifying future Unsatisfactory students?
8. Does the grade inflation pattern persist in 25-26 data? Is it getting worse or better over time?

---

## Run 3 - LEAP State Test Patterns (Feb 22, 2026)

### Key Findings

**(KEY FINDING) Network proficiency peaked in 23-24 and fell back in 24-25.** Three-year trend shows a rise-then-fall pattern, not sustained improvement:
- ELA: 27.1% → 31.0% → 28.4% (+1.3pp net over 2 years, n≈1,820-1,890 per year)
- Math: 16.9% → 22.1% → 19.7% (+2.8pp net, same n)
- Science: 11.5% → 11.0% → 12.0% (+0.5pp net, essentially flat)
The 23-24 gains were not sustained. Any narrative about "upward trajectory" is contradicted by the data.

**(KEY FINDING) LHA is the only school that declined in BOTH ELA and Math over the 2-year window.** LHA ELA: 25.9% → 31.2% → 25.0% (-0.9pp), Math: 21.7% → 27.3% → 20.0% (-1.7pp). The 23-24 spike at LHA was an anomaly. By contrast, Wheatley showed the most consistent improvement: ELA +3.6pp, Math +6.1pp over 2 years.

**(KEY FINDING) 3rd grade took a massive hit in 24-25: ELA -12.3pp, Math -10.3pp year-over-year.** Grade 3 proficiency fell from 37.1% to 24.8% in ELA and from 33.2% to 22.9% in Math (n≈308-319 per year). No other grade declined this sharply. This is the incoming cohort effect — the new 3rd graders tested substantially lower than the prior year's 3rd graders.

**(KEY FINDING) Grade 5 is the Math cliff.** Math proficiency collapses at grade 5 across all schools:
- Ashe: 20.7% (G4) → 9.1% (G5)
- Green: 24.5% → 22.0% (less severe)
- LHA: 19.5% → 15.7%
- Wheatley: 33.3% → 2.4% (catastrophic — n=83, only 2 students proficient)
Grade 5 Math introduces fractions, decimals, and multi-step operations. The content transition is defeating students network-wide.

**(KEY FINDING) SPED and EL students face 15-24 percentage point proficiency gaps.** From _full tables (n=1,371 across 3 schools):
| Group | ELA Prof. | Math Prof. | n |
|---|---|---|---|
| Regular Ed | 32.2% | 21.4% | ~1,200 |
| Special Ed | 11.3% | 6.5% | ~169 |
| Not EL | 32.1% | 20.4% | ~1,230 |
| EL | 8.0% | 11.6% | ~138 |
SPED gap: 20.9pp ELA, 14.9pp Math. EL gap: 24.1pp ELA, 8.8pp Math. Green SPED proficiency is near zero (ELA 2.8%, Math 2.7%, n=36-37).

**(KEY FINDING) MAP BOY percentile is the single strongest LEAP predictor, but only for ELA at scale.** Pearson correlations (scale score → scale score):

| Predictor | LEAP Subject | r | n |
|---|---|---|---|
| MAP BOY %ile | Math SS | **0.838** | 500 |
| MAP BOY %ile | ELA SS | **0.736** | 1,753 |
| anet BOY % | Math SS | **0.694** | 1,738 |
| anet EOY % | ELA SS | 0.667 | 1,820 |
| anet EOY % | Math SS | 0.685 | 1,795 |
| MAP BOY RIT | ELA SS | 0.650 | 1,753 |
| anet BOY % | ELA SS | 0.633 | 1,756 |
| MAP BOY RIT | Math SS | 0.551 | 500 |

MAP Math has the highest r (0.838) but only 500 students have MAP BOY Math scores (26.5% coverage). For practical use at scale, anet BOY Math (r=0.694, n=1,738) is the best Math LEAP predictor with adequate coverage.

**(KEY FINDING) anet BOY < 30% catches ~81-83% of future LEAP Unsatisfactory students.** Threshold analysis:

| Subject | Threshold | Sensitivity | PPV | Interpretation |
|---|---|---|---|---|
| ELA | <25% | 68.6% | 35.7% | Misses 1 in 3 Unsat students |
| ELA | <30% | 80.6% | 32.2% | Best balance for ELA |
| ELA | <35% | 87.6% | 30.0% | High catch rate, more false positives |
| Math | <25% | 68.0% | 42.5% | Higher PPV but misses too many |
| Math | <30% | 82.9% | 40.0% | Best balance for Math |
| Math | <35% | 89.6% | 36.7% | Very high sensitivity |

Recommended intervention threshold: anet BOY < 30% for both subjects. This catches 4 in 5 future Unsatisfactory students while flagging a manageable number (~700-740 students network-wide).

**(KEY FINDING) Internal assessments cleanly separate LEAP achievement levels.** Mean anet BOY % by LEAP outcome:

| LEAP Level | anet BOY ELA% | anet BOY Math% | MAP BOY ELA %ile | MAP BOY Math %ile |
|---|---|---|---|---|
| Unsatisfactory | 21.6% | 21.4% | 11.6 | 10.2 |
| Approaching Basic | 28.6% | 30.2% | 19.4 | 25.9 |
| Basic | 41.0% | 46.8% | 36.6 | 46.3 |
| Mastery | 57.5% | 64.3% | 57.4 | 69.3 |
| Advanced | 72.2% | 77.5% | 74.0 | 89.4 |

Monotonic increase across all instruments. Effect sizes are large (Cohen's d > 2.0 between Unsatisfactory and Mastery). These instruments are measuring the same underlying construct that LEAP measures.

**(NULL FINDING) Science proficiency is flat at ~11-12% across all three years.** Despite being a tested subject since at least 22-23, there has been zero improvement. This is not a measurement artifact — it reflects genuine stagnation. Only Green showed any Science improvement (14.0% → 17.5%, +3.5pp in 24-25).

### LEAP Connect (Alternate Assessment)

- 41 students in 24-25 (up from 37 in 23-24)
- Distribution: Green 13, LHA 12, Wheatley 9, Ashe 7
- Predominantly students with Autism (16/41 = 39%) and Mild Mental Disability (12/41 = 29%)
- 78% male, 80.5% Black
- Uses different achievement levels: Above Goal, At Goal, Near Goal, Below Goal
- Most students (27/41 = 66%) are At Goal or Above Goal in both ELA and Math
- These 41 students are NOT included in the regular LEAP totals above

### Bottom 25th Percentile (sps.24_25_bottom_25)

- Table contains 2,856 students (broader than LEAP-tested population — includes K-2)
- 588 students flagged as Bottom 25th (20.6%)
- ELA: 338 flagged, Math: 335 flagged, DIBELS: 113, DIBELS+ELA: 451
- Distribution is fairly even across schools (19.2-22.2%)
- Grade distribution reveals a structural artifact: 3rd grade has almost no flags (1.6%) because it's the first LEAP testing year — there's no prior-year LEAP data to flag against
- 4th grade has the highest flag rate (34.3%, n=303) — these are students who tested poorly in their first LEAP year (3rd grade)
- Grades 5-7 cluster at 30-33% flagged

### 24-25 Full Achievement Distribution (Network Level)

| Subject | U | AB | B | M | A | Prof% | n |
|---|---|---|---|---|---|---|---|
| ELA | 16.9% | 25.4% | 29.3% | 25.3% | 3.1% | 28.4% | 1,887 |
| Math | 21.7% | 31.9% | 26.8% | 18.3% | 1.4% | 19.7% | 1,890 |
| Science | 28.7% | 34.1% | 25.1% | 11.1% | 0.9% | 12.0% | 1,890 |
| Soc. Studies | 28.7% | 33.1% | 24.4% | 11.7% | 2.1% | 13.8% | 1,890 |

### Scale Score Statistics (24-25)

| School | ELA Mean (SD) | Math Mean (SD) | Sci Mean (SD) |
|---|---|---|---|
| Ashe | 739 (32) | 726 (30) | 717 (27) |
| Green | 730 (33) | 725 (29) | 717 (31) |
| LHA | 729 (32) | 725 (31) | 712 (29) |
| Wheatley | 726 (34) | 721 (30) | 714 (27) |

### Schema Gotchas (New)

1. **(CRITICAL) LEAP _full tables exist for 399001, 399002, 399004 but NOT 399005 (LHA).** Demographics analysis from _full tables covers only 3 of 4 schools (n=1,371 of 1,890). LHA demographics require a different data source.
2. **(CRITICAL) LEAP _full tables have domain/standard subscores as separate columns with grade-specific column names.** Column names like `FractionsasNumbersEquivalence`, `ReadingLiteraryText` vary by grade. These cannot be easily UNION'd across grades without careful schema alignment.
3. **(CRITICAL) LEAP _full tables use `EducationClassificationSummary` (values: "Regular", "Special") not `SpecialEducationExceptionalityCategory` for SPED status.** The latter is only in LEAP Connect.
4. **(CRITICAL) 22-23 LEAP tables exist and are queryable.** `399001LEAPData22_23` through `399005LEAPData22_23`. This enables 3-year trend analysis.
5. Grade encoding differs by year: 23-24 uses zero-padded strings ("03"), 24-25 uses unpadded strings ("3"). Must normalize with `SAFE_CAST(Grade AS INT64)` for cross-year comparisons.
6. LEAP _full has `LAPEconomicallyDisadvantaged` (no space, no backtick needed) while LEAP Connect has `LAP Economically Disadvantaged` (with spaces, needs backticks).
7. LEAP _full has `RemediationNeeded` column: "Yes" for 829/1371 (60.5%) of students. This is a useful flag.
8. LEAP Connect uses different achievement levels (At Goal/Above Goal/Near Goal/Below Goal) than regular LEAP (Unsatisfactory/Approaching Basic/Basic/Mastery/Advanced). These are NOT comparable.
9. Bottom 25th percentile table (`sps.24_25_bottom_25`) uses `Student_Number` as STRING, matching roster. Join works directly without SAFE_CAST.
10. Bottom 25th table covers K-8 (includes Grade 0 = K and Grade -1 = Pre-K). Flag rates for K-2 are based on DIBELS, not LEAP.

### Dead Ends

1. **Trying to UNION _full tables across all 4 schools**: 399005 (LHA) has no _full table. Must handle LHA demographics separately.
2. **Social Studies trend analysis**: Social Studies data has NULL values in 23-24 data and inconsistent availability across years. Not reliable for trend analysis.
3. **Bottom 25th as a LEAP alignment variable**: The flags are binary (Yes/blank) with no underlying score. Useful for identifying students but not for correlation or concordance analysis.

### Questions for Run 4
1. What is the optimal composite predictor (anet + MAP combined) for LEAP? Does a weighted combination outperform either alone?
2. For students where MAP and anet disagree on predicted proficiency, which instrument is more accurate?
3. Can we build classification thresholds (not just for Unsatisfactory, but for each LEAP level) from anet and MAP?
4. What explains the Grade 3 collapse in 24-25? Is it a cohort effect (weaker incoming students) or a testing/instruction issue?
5. What explains the Wheatley Grade 5 Math catastrophe (2.4% proficiency, n=83)? Is this a staffing/curriculum issue or a data artifact?
6. Do anet domain-level scores predict LEAP domain subscores? (The _full tables have ELA Reading/Writing subscores and Math domain subscores.)
7. How do SPED students perform on internal assessments vs LEAP? Is the gap present pre-LEAP or does it widen at the state test?
8. For the 41 LEAP Connect students, do they appear in the regular assessment systems (anet, MAP)? What does their internal data look like?

---

## Run 4 - Concordance Analysis (Feb 22, 2026)

### Key Findings

**(KEY FINDING) anet interims agree with LEAP on 82-86% of students; course grades agree on only 77%.** At optimal thresholds, anet Math MOY has the highest concordance (86.1%, kappa=0.488, n=1,700), followed by anet Math EOY (85.4%, kappa=0.549, n=1,792). Course grades require a 90% (A-) threshold just to reach 77% concordance. Below A-, grades systematically over-identify proficiency.

| System | Best Threshold | Concordance | Kappa | n |
|---|---|---|---|---|
| MAP Math %ile | 60th | 89.4% | 0.642 | 499 |
| MAP Math proj | M/A | 88.4% | 0.633 | 499 |
| anet Math MOY | 65% | 86.1% | 0.488 | 1,700 |
| anet Math EOY | 55% | 85.4% | 0.549 | 1,792 |
| anet Math BOY | 65% | 84.3% | 0.478 | 1,735 |
| anet ELA EOY | 60% | 81.6% | 0.519 | 1,820 |
| MAP ELA %ile | 60th | 81.4% | 0.501 | 1,753 |
| MAP ELA proj | M/A | 81.3% | 0.481 | 1,753 |
| anet ELA BOY | 60% | 79.6% | 0.466 | 1,756 |
| anet ELA MOY | 45% | 77.9% | 0.446 | 1,744 |
| Grade ELA | 90% | 77.7% | 0.370 | 1,862 |
| Grade Math | 90% | 76.6% | 0.333 | 1,867 |

**(KEY FINDING) 451 students (24.2%) carry B+ Math grades but score Unsatisfactory or Approaching Basic on LEAP.** In ELA, 234 students (12.6%) have the same mismatch. These students and families receive a false signal of proficiency from report cards.

| School | ELA Inflated | ELA Rate | Math Inflated | Math Rate |
|---|---|---|---|---|
| Ashe | 61/529 | 11.5% | 153/528 | 29.0% |
| LHA | 110/511 | 21.5% | 145/514 | 28.2% |
| Green | 26/328 | 7.9% | 67/330 | 20.3% |
| Wheatley | 37/494 | 7.5% | 86/495 | 17.4% |
| **Network** | **234/1,862** | **12.6%** | **451/1,867** | **24.2%** |

**(KEY FINDING) When anet flags at-risk but grades say "B or better," anet is right 60-73% of the time.** Of 339 students with anet Math <30% but Grade >=80%, 246 (72.6%) scored U/AB on LEAP. In ELA, 122/202 (60.4%). The anet correctly identified the risk; the grade was a false positive. These 246 Math students represent the highest-confidence intervention targets.

**(KEY FINDING) Ashe Math grades have near-zero correlation with LEAP (r=0.147, kappa=0.063).** This is the lowest correlation and concordance of any system at any school. A student's Math course grade at Ashe explains only 2.2% of the variance in their LEAP Math performance. By contrast, Green Math grades show the best alignment (r=0.695, kappa=0.191).

| School | anet ELA EOY r | anet Math EOY r | MAP ELA %ile r | Grade ELA r | Grade Math r |
|---|---|---|---|---|---|
| Ashe | 0.638 | 0.694 | 0.757 | 0.568 | **0.147** |
| Green | 0.700 | 0.726 | 0.726 | 0.615 | 0.695 |
| LHA | 0.678 | 0.680 | 0.730 | 0.544 | 0.531 |
| Wheatley | 0.640 | 0.679 | 0.721 | 0.588 | 0.437 |

**(KEY FINDING) Grades compress performance into a narrow band — 14-18pp spread vs 51-53pp for anet.** Mean scores by LEAP level:

| LEAP Level | anet EOY ELA | anet EOY Math | Grade ELA | Grade Math |
|---|---|---|---|---|
| Unsatisfactory | 23.2% | 23.4% | 72.9% | 75.5% |
| Approaching Basic | 30.7% | 29.5% | 76.7% | 79.8% |
| Basic | 43.9% | 42.5% | 80.7% | 85.1% |
| Mastery | 60.1% | 61.3% | 86.3% | 87.4% |
| Advanced | 73.9% | 76.6% | 90.6% | 89.4% |
| **Spread (Adv-Unsat)** | **50.7pp** | **53.2pp** | **17.7pp** | **13.9pp** |

anet separates LEAP levels by 50+ pp. Grades separate them by only 14-18pp. Grades give Unsatisfactory students a C+ (73-76%) and Advanced students a B+/A- (89-91%). This 14pp gap is smaller than the standard deviation within any single level.

**(KEY FINDING) Grade 6 Math is the inflation epicenter: 57-60% inflation at two schools.** Wheatley G6 Math: 60% inflated (50/84). Ashe G6 Math: 57% (52/92). LHA G4 Math: 57% (46/81). These grade-school combinations represent the biggest disconnects between classroom assessment and state standards.

**(KEY FINDING) MAP Math %ile is the single strongest per-student LEAP predictor (r=0.840) but covers only 26.4% of students.** MAP Math BOY has only 499 matches to LEAP (vs 1,700+ for anet). For practical use at scale, anet Math MOY is the best predictor with adequate coverage (r=0.724, n=1,700).

### Correlation Ranking (all internal measures vs LEAP scale scores)

| Predictor | LEAP Subject | r | n |
|---|---|---|---|
| MAP Math %ile | Math | **0.840** | 499 |
| MAP ELA %ile | ELA | **0.732** | 1,753 |
| anet Math MOY | Math | 0.724 | 1,700 |
| anet Math BOY | Math | 0.693 | 1,735 |
| anet Math EOY | Math | 0.685 | 1,792 |
| anet ELA EOY | ELA | 0.667 | 1,820 |
| MAP ELA RIT | ELA | 0.649 | 1,753 |
| anet ELA BOY | ELA | 0.633 | 1,756 |
| anet ELA MOY | ELA | 0.581 | 1,744 |
| Grade ELA | ELA | 0.564 | 1,862 |
| MAP Math RIT | Math | 0.550 | 499 |
| Grade Math | Math | **0.396** | 1,867 |

### School-Level Concordance (at optimal thresholds)

| School | anet ELA Conc/k | anet Math Conc/k | Grade ELA Conc/k | Grade Math Conc/k |
|---|---|---|---|---|
| Ashe | 75.7%/0.453 | 84.3%/0.495 | 65.4%/0.356 | 40.2%/0.063 |
| Green | 80.4%/0.468 | 85.6%/0.561 | 74.7%/0.464 | 52.4%/0.191 |
| LHA | 85.9%/0.602 | 84.2%/0.556 | 57.1%/0.261 | 50.6%/0.174 |
| Wheatley | 84.4%/0.533 | 87.5%/0.590 | 73.5%/0.388 | 65.7%/0.268 |

### Data Quality Notes

1. 1,972 rows extracted from 4 LEAP tables; 85 LASID duplicates removed (students appearing in multiple school tables), yielding 1,887 unique students.
2. Coverage: anet 91-97%, MAP ELA 93%, MAP Math 26%, Grades 99%.
3. Grades joined via roster bridge: grades.Student_Number -> roster.Student_Number -> roster.State_StudentNumber (LASID). Match rate: 98.7-98.9%.
4. Course names are clean: "ELA 3" through "ELA 8", "Math 3" through "Math 8", "Math Discovery", "Algebra I". Y1 grading term used.
5. All analysis uses 2024-25 data (the most recent complete year with full cross-system coverage, per Runs 1-3).

### Schema Gotchas (New)

1. **(CRITICAL) 85 students appear in multiple LEAP school tables.** These are likely transfers between FirstLine schools during the year. Must deduplicate by LASID to avoid double-counting.
2. **(CRITICAL) Optimal grade threshold for concordance is 90% (A-), not 80% (B).** Using 80% as "proficient" would yield even worse concordance (~73-74%).
3. Course grade table `current_grades_2024_2025` has clean course names that match grade levels directly (e.g., "ELA 6", "Math 8"). No fuzzy matching needed.
4. "Math Discovery" appears as a course for some students (n=87) — likely a remediation or enrichment track. "Algebra I" (n=56) is the 8th grade advanced track.

### Dead Ends

1. **Using B (80%) as the grade proficiency threshold**: This was the intuitive choice but yields terrible concordance (73-74%). The data shows grades must be at A- (90%) to have any predictive alignment with LEAP — which itself is evidence of inflation.
2. **MAP Math as a practical predictor**: Despite the highest r (0.840), only 499 of 1,887 LEAP students have MAP Math BOY scores. Cannot be used for network-wide risk identification without expanding administration.
3. **anet ELA MOY as a predictor**: The optimal threshold (45%) with concordance of only 77.9% makes MOY ELA the weakest anet cycle. This aligns with Run 2's finding that anet ELA scores drop from BOY to MOY due to harder content.

### Questions for Run 5
1. Can a composite predictor (anet + MAP) outperform either alone? What weights optimize sensitivity for identifying Unsatisfactory students?
2. What does the grade inflation look like at the teacher level? Are specific teachers driving the school-level inflation, or is it systemic?
3. Do SPED and EL students show different concordance patterns? Is the grade inflation worse or better for these subgroups?
4. For the 246 "triple misalignment" Math students (Grade B+, anet <30%, LEAP U/AB), what are their demographics? Are they concentrated in specific subgroups?
5. How stable are these concordance rates across years? Does 23-24 data show similar patterns?
6. What would a "calibrated grade" look like — if we adjusted grade thresholds to match LEAP proficiency rates by school and grade?
7. For the 85 LASID duplicates across school tables, are these transfers? Which school's LEAP result should be used?
8. Does the anet domain-level data (standard codes) predict specific LEAP subscores from the _full tables?

---

## Run 5 - Synthesis & Predictive Model (Feb 22, 2026)

### Key Findings

**(KEY FINDING) The composite predictor (anet + MAP) offers marginal gains over MAP alone.** Optimal weights: ELA = 35% anet + 65% MAP (r=0.759, n=1,808), Math = 30% anet + 70% MAP (r=0.850, n=500). Composite gains over best single predictor: +0.029 (ELA), +0.017 (Math). The improvement does not justify the added complexity. **Recommendation: use anet BOY as the primary network-wide early-warning instrument; supplement with MAP percentile where available.**

**(KEY FINDING) Risk tiers validated with large effect sizes.** Four-tier framework based on anet BOY percent correct:

| Tier | Threshold | ELA n | ELA Prof% | Math n | Math Prof% |
|---|---|---|---|---|---|
| Crisis | <20% | 412 | 4.4% | 408 | 1.2% |
| Intervention | 20-30% | 342 | 7.6% | 345 | 5.2% |
| Watch | 30-50% | 477 | 26.0% | 461 | 13.9% |
| On Track | ≥50% | 609 | 59.1% | 573 | 50.4% |

Cohen's d between Crisis and On Track: 1.87 (ELA), 2.20 (Math). Between Intervention and Watch: 0.69 (ELA), 0.67 (Math). These are practically significant separations.

**(KEY FINDING) SPED students face the most inflated grade signals in the network.** From 3 schools with demographic data (n=1,385):
- SPED: Mean Math grade 79.9%, LEAP Math proficiency 6.5% — gap of 73.4pp
- Regular Ed: Mean Math grade 82.3%, LEAP Math proficiency 21.5% — gap of 60.8pp
- SPED Math grade inflation rate: 37.0% vs 20.9% for regular ed (16.1pp worse)
- Green SPED Math inflation: 51.4% (18/35 students)
- SPED risk tier distribution: 42% Crisis + 25% Intervention = 67% at-risk (ELA)

**(KEY FINDING) EL students show largest ELA gap but anet correctly identifies them.** EL LEAP ELA proficiency: 2.2% (n=91). EL mean anet BOY ELA: 22.2%. The anet correctly places nearly all EL students in Crisis or Intervention tiers. The grade system gives them a C+ average (73.5%), masking the severity of the gap.

**(KEY FINDING) Grade 6 Math is the inflation epicenter: 43.7% of 6th graders have inflated Math grades.** Grade-level Math inflation rates: G6 43.7%, G4 27.6%, G8 20.6%, G5 19.8%, G3 17.9%, G7 13.7%. Grade 6 is 2-3x worse than the median grade level.

**(KEY FINDING) Gender patterns in concordance: Males score worse on ELA, females do not.** Males: 26.1% ELA proficient, 44.9% anet ELA at-risk. Females: 32.0% ELA proficient, 35.7% at-risk. Math shows no gender gap (20.3% vs 20.5% proficiency). Males have higher Math inflation rate (20.9% vs 26.3% — actually females are slightly higher), but males receive more at-risk flags.

**(NULL FINDING) The composite predictor does NOT justify operational investment.** At optimal weights, the composite (anet + MAP) improves r by only 0.017-0.029 over the best single predictor. This is statistically detectable but practically meaningless. A school leader using anet BOY alone loses almost nothing compared to a composite model.

### Full Correlation Table (All Predictors vs LEAP Scale Score)

| Rank | Predictor | LEAP Subject | r | r² | n | Coverage |
|---|---|---|---|---|---|---|
| 1 | MAP BOY %ile | Math | 0.836 | 0.698 | 511 | 26% |
| 2 | MAP BOY %ile | ELA | 0.731 | 0.535 | 1,838 | 93% |
| 3 | anet MOY % | Math | 0.721 | 0.520 | 1,724 | 88% |
| 4 | anet EOY % | Math | 0.689 | 0.475 | 1,843 | 94% |
| 5 | anet BOY % | Math | 0.687 | 0.472 | 1,787 | 92% |
| 6 | anet EOY % | ELA | 0.667 | 0.444 | 1,904 | 97% |
| 7 | MAP BOY RIT | ELA | 0.652 | 0.424 | 1,838 | 93% |
| 8 | anet BOY % | ELA | 0.633 | 0.401 | 1,840 | 93% |
| 9 | anet MOY % | ELA | 0.579 | 0.336 | 1,828 | 93% |
| 10 | Grade | ELA | 0.550 | 0.303 | 1,955 | 99% |
| 11 | MAP BOY RIT | Math | 0.550 | 0.302 | 511 | 26% |
| 12 | Grade | Math | 0.411 | 0.169 | 1,937 | 99% |

### Risk Tier Distribution by School

| Subject | School | Crisis | Intervention | Watch | On Track | n |
|---|---|---|---|---|---|---|
| ELA | Ashe | 22% | 16% | 24% | 37% | 537 |
| ELA | Green | 19% | 18% | 26% | 37% | 314 |
| ELA | LHA | 22% | 18% | 27% | 33% | 491 |
| ELA | Wheatley | 26% | 22% | 26% | 26% | 498 |
| Math | Ashe | 24% | 17% | 30% | 29% | 487 |
| Math | Green | 20% | 22% | 28% | 30% | 313 |
| Math | LHA | 21% | 19% | 17% | 43% | 509 |
| Math | Wheatley | 26% | 21% | 28% | 25% | 478 |

Wheatley has highest Crisis+Intervention concentration (48% ELA, 47% Math). LHA has highest On Track Math (43%).

### SPED/EL Concordance Detail

| Group | n | Mean anet BOY ELA | Mean anet BOY Math | Mean Grade ELA | Mean Grade Math | LEAP ELA Prof | LEAP Math Prof |
|---|---|---|---|---|---|---|---|
| SPED | 170 | 27.0% | 30.3% | 76.8% | 79.9% | 11.2% | 6.5% |
| Regular | 1,215 | 41.6% | 39.1% | 79.5% | 82.3% | 32.2% | 21.5% |
| EL | 91 | 22.2% | 31.3% | 73.5% | 77.9% | 2.2% | 8.8% |
| Not EL | 1,294 | 41.1% | 38.5% | 79.6% | 82.3% | 31.5% | 20.4% |

SPED anet-grade gap (Math): anet says 30.3%, grade says 79.9% — a 49.6pp disconnect.
EL anet-grade gap (ELA): anet says 22.2%, grade says 73.5% — a 51.3pp disconnect.

### Schema Gotchas (New)

1. **(CRITICAL) anet demographic fields (sped_status, ell_status, frl_status) are all coded "0" for every student in the 24-25 data.** These fields are non-functional. Use LEAP _full `EducationClassificationSummary` or support roster `Limited_English_Proficiency` for demographics instead.
2. **(CRITICAL) anet `race` field has NULL for 611/1,887 students (32%).** Race data in anet is incomplete. The LEAP _full tables have more complete demographics but only for 3 of 4 schools.
3. LEAP _full tables: `EducationClassificationSummary` values are "Regular" and "Special" (not "Yes"/"No" or "IEP"/"None").
4. Student support roster: `Limited_English_Proficiency` = "Yes" or NULL (not "Y"/"N").
5. Student support roster: `eSER_Primary_Exceptionality` has specific values like "Talented", "Autism", etc. — use for disability category analysis.
6. Python on MINGW64/Windows: `/tmp/` paths in bash do not resolve in Python. Must copy files to Windows-accessible paths (`C:/Users/...`) before Python processing.

### Dead Ends

1. **anet demographic fields for SPED/EL analysis**: All values are "0" — the fields are not populated. Wasted a query round before discovering this. Use LEAP _full or support roster instead.
2. **Composite predictor as operational tool**: r gain of 0.017-0.029 does not justify the complexity of maintaining a composite scoring system. Stick with single predictors.
3. **Grade-level risk tier analysis via Python**: The `grade` field from SAFE_CAST returned as STRING in JSON, not INT64. Required string comparison (e.g., `r.get('grade') == '6'` not `== 6`). Fixed but wasted debugging time.

### School-Specific Synthesis

**Ashe**: Math grading is broken (r=0.147, functionally random). 29% Math inflation rate. Priority: rebuild Math grading around standards-referenced criteria.

**Green**: Best grade alignment in the network (Math r=0.695). But SPED Math inflation is catastrophic (51.4%). Highest Math proficiency (24.0%). Green's practices should be studied and shared.

**LHA**: Only school to decline in both subjects over 2 years. Worst ELA inflation (21.5%). But strongest Math pipeline (43% On Track). 23-24 gains were anomalous.

**Wheatley**: Highest concentration of at-risk students (48% Crisis+Intervention in ELA). Grade 5 Math: 2.4% proficiency (2/83). But most consistent 2-year improvement trajectory (+3.6pp ELA, +6.1pp Math).

### Questions for Future Studies

1. **Prospective validation**: Apply these risk tiers to 25-26 BOY data and compare to eventual 25-26 LEAP results. Do the tier proficiency rates hold?
2. **Teacher-level inflation**: Which individual teachers show the largest grade-LEAP disconnect? Is inflation concentrated or systemic?
3. **Domain-level prediction**: Do anet standard-level (domain) scores predict LEAP subscores from _full tables? Which domains are most predictive?
4. **Cross-year stability**: Does the 23-24 data produce similar correlations and concordance rates? Is the Grade 6 Math inflation a persistent structural issue?
5. **K-2 pipeline**: Can DIBELS BOY scores predict Grade 3 LEAP outcomes? How early can the early-warning system start?
6. **Intervention effectiveness**: For students flagged as Crisis/Intervention who received targeted support, did outcomes improve? Need intervention tracking data.
7. **Science assessment alignment**: Why is Science proficiency flat at 12%? What internal assessments exist for Science and do any predict LEAP Science?
8. **Grade 5 Math content analysis**: What specific Math standards are causing the G4→G5 proficiency cliff? Can anet domain-level data identify the specific skills gap?

---

## Run 6 - Cross-Subject Correlations & Standards-Level Drill-Down (Feb 22, 2026)

### Key Findings

**(KEY FINDING) ELA predicts Science marginally better than Math (r=0.729 vs r=0.702, n=1,887), but the difference is not statistically significant (Fisher z=1.69, p=0.09).** Both are strong cross-subject predictors. ELA's slight edge likely reflects the reading comprehension demands of science test questions. All four LEAP subjects are strongly correlated (r range: 0.674-0.773). ELA-Social Studies is the strongest pair (r=0.773).

**(KEY FINDING) anet ELA BOY predicts LEAP Science (r=0.614) and Social Studies (r=0.635) nearly as well as LEAP ELA itself (r=0.633).** No separate Science or Social Studies screening instrument is needed. The existing anet ELA early-warning threshold (<30%) should be treated as a pan-subject risk flag.

**(KEY FINDING) Performance Matters Science is the only direct Science predictor in any internal system: r=0.641 with LEAP Science (n=1,848, 97.9% coverage).** PM Science combines Standards Checkpoints (n=1,628), Tests (n=1,834), and End of Module assessments (n=80). Mean PM Science score is 40.8%. Strongest at Green (r=0.737) and Grade 5 (r=0.734), weakest at Grade 8 (r=0.524).

**(KEY FINDING) "Reading Informational Text" is the single most broadly predictive anet domain: r >= 0.60 with all four LEAP subjects (ELA 0.652, Math 0.606, Science 0.646, Social Studies 0.677, n=1,629).** This is the one domain where improvement would lift all four LEAP subjects.

**(KEY FINDING) Number sense domains (Base Ten, Fractions, Number System) explain 49-58% of LEAP Math variance -- far more than procedural domains.** Top Math domain predictors of LEAP Math:
- Number and Operations in Base Ten: r=0.763 (n=627)
- Number and Operations -- Fractions: r=0.730 (n=482)
- The Number System: r=0.701 (n=557)
Bottom Math domains: Measurement & Data (r=0.540), Statistics & Probability (r=0.342).

**(KEY FINDING) LEAP ELA "Reading Performance" subscore is the strongest predictor of LEAP Science (r=0.691, n=1,368).** ELA subscores predict Science scale score at: Reading Performance 0.691, Reading Literary Text 0.648, Reading Informational Text 0.597, Vocabulary 0.537, Writing Performance 0.468. Science depends on reading comprehension, not writing ability.

**(KEY FINDING) Green is the only school where Math predicts Science better than ELA does (r=0.653 vs r=0.604).** At all other schools, ELA is the stronger Science predictor (gap of 0.06-0.19). Something about Green's Math instruction builds transferable scientific reasoning.

**(NULL FINDING) Performance Matters has zero Social Studies assessment data in 2024-25.** Social Studies has no internal predictor at any school. The only prediction path is cross-subject via anet ELA (r=0.635).

**(NULL FINDING) Science subscores (Reason Scientifically, Investigate) have severe floor effects: 63% of students score "Weak" on both.** This attenuates ordinal correlations and limits the discriminating power of subscore-level analysis. The true Science-ELA relationship may be stronger than measured.

### LEAP 4x4 Cross-Subject Correlation Matrix (n=1,887)

|  | ELA | Math | Science | Soc. Studies |
|---|---|---|---|---|
| ELA | 1.000 | 0.674 | 0.729 | 0.773 |
| Math | 0.674 | 1.000 | 0.702 | 0.688 |
| Science | 0.729 | 0.702 | 1.000 | 0.766 |
| Soc. Studies | 0.773 | 0.688 | 0.766 | 1.000 |

### Cross-Subject Correlations by School

| School | n | ELA-Math | ELA-Sci | Math-Sci | ELA-SS | Math-SS | Sci-SS |
|---|---|---|---|---|---|---|---|
| Ashe | 534 | 0.716 | 0.744 | 0.734 | 0.796 | 0.705 | 0.775 |
| Green | 336 | 0.701 | 0.734 | 0.759 | 0.724 | 0.704 | 0.789 |
| LHA | 519 | 0.685 | 0.735 | 0.685 | 0.770 | 0.724 | 0.756 |
| Wheatley | 498 | 0.606 | 0.717 | 0.654 | 0.768 | 0.634 | 0.759 |

### Cross-Subject Correlations by Grade

| Grade | n | ELA-Sci | Math-Sci | ELA-SS | Sci-SS |
|---|---|---|---|---|---|
| 3 | 319 | 0.738 | 0.673 | 0.780 | 0.763 |
| 4 | 294 | 0.777 | 0.726 | 0.794 | 0.799 |
| 5 | 313 | 0.787 | 0.762 | 0.808 | 0.810 |
| 6 | 325 | 0.709 | 0.729 | 0.743 | 0.719 |
| 7 | 315 | 0.746 | 0.731 | 0.759 | 0.798 |
| 8 | 321 | 0.743 | 0.721 | 0.759 | 0.791 |

Grade 5 shows the strongest cross-subject correlations. Grade 6 is weakest, suggesting the middle school transition disrupts alignment.

### anet Domain Taxonomy (2024-25)

ELA Domains: Reading Informational Text (1,633), Reading Literature (1,564), Vocabulary Interpretation (859).

Math Domains: Expressions and Equations (855), Measurement and Data (799), Operations and Algebraic Thinking (704), Number and Operations in Base Ten (628), Geometry (588), The Number System (560), Number and Operations -- Fractions (484), Ratios and Proportional Relationships (457), Statistics and Probability (403), Functions (259).

Math domains are grade-specific: Base Ten and Measurement/Data are elementary (G3-5), Expressions/Equations and The Number System are middle school (G6-8).

### anet Domain -> LEAP Scale Score: Top Predictors

| Domain | LEAP Subject | r | r-squared | n |
|---|---|---|---|---|
| ELA Reading Info Text | Soc. Studies | 0.677 | 0.459 | 1,629 |
| ELA Reading Info Text | ELA | 0.652 | 0.425 | 1,629 |
| ELA Reading Info Text | Science | 0.646 | 0.417 | 1,629 |
| ELA Reading Literature | ELA | 0.640 | 0.410 | 1,559 |
| ELA Reading Literature | Soc. Studies | 0.636 | 0.405 | 1,559 |
| ELA Reading Literature | Science | 0.630 | 0.397 | 1,559 |
| Math Num Ops Base Ten | Math | 0.763 | 0.583 | 627 |
| Math Num Ops Fractions | Math | 0.730 | 0.533 | 482 |
| Math The Number System | Math | 0.701 | 0.491 | 557 |
| Math Ops Algebraic Thinking | Math | 0.673 | 0.452 | 702 |

### Weakest anet Domains

| Domain | LEAP Math r | LEAP Sci r | n |
|---|---|---|---|
| Statistics & Probability | 0.342 | 0.283 | 403 |
| Measurement & Data | 0.540 | 0.255 | 797 |
| Functions | 0.505 | 0.293 | 259 |

### LEAP ELA Subscore -> Science Scale Score (n=1,368)

| ELA Subscore | r with Sci SS | Mean Sci at Weak | Mean Sci at Strong | Spread |
|---|---|---|---|---|
| Reading Performance | 0.691 | 695.7 | 742.6 | +46.9 |
| Reading Literary Text | 0.648 | 697.2 | 739.5 | +42.3 |
| Reading Informational Text | 0.597 | 699.8 | 740.2 | +40.4 |
| Vocabulary | 0.537 | 701.4 | 736.1 | +34.7 |
| Writing Performance | 0.468 | 700.2 | 728.4 | +28.2 |

### Schema Gotchas (New)

1. **(CRITICAL) LEAP _full table name pattern is `399001LEAPData24_25_full` (suffix `_full`), NOT `399001LEAPDataFull24_25`.** The data dictionary / prior runs had the wrong pattern. Only 3 schools have _full tables: 399001, 399002, 399004.
2. **(CRITICAL) LEAP _full subscore columns are CATEGORICAL (Weak/Moderate/Strong), not numeric.** Must convert to ordinal (1/2/3) for correlation analysis.
3. **(CRITICAL) Science subscores have severe floor effects: 62-64% score "Weak."** This attenuates correlations involving these variables.
4. **(CRITICAL) anet domain "Number and Operations -- Fractions" appears with TWO encodings: em-dash and hyphen.** "Number and Operations--Fractions" (n=484) vs "Number and Operations-Fractions" (n=207). These are different student populations (likely different assessment cycles or grade levels). Must combine or handle separately.
5. PM Science `Assessment_Category` values: "Standards Checkpoint" (n=1,628), "Test" (n=1,834), "End of Module" (n=80), "Quiz" (n=1,873), and `\N` (n=423). Quizzes were excluded from the LEAP correlation analysis.
6. No Social Studies data exists anywhere in Performance Matters for 2024-25.
7. Math domains are grade-specific: domain coverage varies from 207 (Number and Operations-Fractions) to 855 (Expressions and Equations). Cross-grade domain comparisons require careful interpretation.

### Dead Ends

1. **INFORMATION_SCHEMA queries**: Still not permitted. Used `bq ls` to find table names.
2. **Direct numeric Science subscores from _full tables**: The values are categorical strings ("Weak", "Moderate", "Strong"), not numeric. Ordinal encoding (1-3) is a workaround but limits correlation analysis precision.
3. **Social Studies internal assessment data**: Does not exist in any internal system (anet, MAP, PM). Social Studies is truly unmonitored.
4. **Python file paths on MINGW64**: `/c/Users/...` paths fail in Python `open()`. Must use `C:/Users/...` format for Windows-compatible paths.

### Questions for Future Studies

1. **Can PM Science Standards Checkpoints alone (excluding Tests/EOM) predict LEAP Science better than the combined PM score?** Checkpoints may be more standardized across schools.
2. **Does the anet ELA -> LEAP Science relationship hold for 25-26?** Prospective validation with 25-26 BOY data and eventual 25-26 LEAP results.
3. **What explains Green's unique Math -> Science prediction pattern?** Is it a curriculum difference, staffing difference, or statistical artifact?
4. **Can domain-level anet scores be used to create differentiated intervention groups?** E.g., students weak on Reading Informational Text but strong on Reading Literature may need different interventions than students weak on both.
5. **Should PM Science Standards Checkpoints be elevated to a formal screening tool?** With r=0.641, it performs comparably to anet ELA as a Science predictor.
6. **Can a combined anet ELA + PM Science composite improve Science prediction beyond either alone?** Similar to the Run 5 composite analysis but for Science.
7. **What explains the Grade 8 weakness in PM Science -> LEAP Science prediction (r=0.524)?** Are 8th grade PM assessments misaligned with LEAP content, or is this a sample composition issue?
8. **Is the Grade 5 "sweet spot" for cross-subject correlation (r=0.787 ELA-Sci) an artifact of the G5 content or a genuine developmental pattern?**

---

## Run 7 - PM Full-Subject, Fluency/ORF, & Cross-Year Validation (Feb 22, 2026)

### Key Findings

**(KEY FINDING) Performance Matters is the strongest LEAP predictor in the network — stronger than anet interims.** PM Math correlates with LEAP Math at r=0.829 (n=1,883), exceeding anet Math BOY (r=0.687), anet Math MOY (r=0.724), and rivaling MAP Math percentile (r=0.840) with 3.7x the coverage. PM English correlates at r=0.753 (n=1,883) vs anet ELA EOY at r=0.667. On the SAME students (n=1,839-1,844), PM outperforms anet in head-to-head: PM Math r=0.830 vs anet r=0.765 (+0.065); PM ELA r=0.756 vs anet r=0.733 (+0.023).

**(KEY FINDING) PM provides the ONLY internal predictors for Science and Social Studies.** PM Science → LEAP Science: r=0.714 (n=1,876). PM Social Studies → LEAP Social Studies: r=0.741 (n=1,881). No other internal system (anet, MAP, grades) assesses these subjects at scale. These are the first Science and Social Studies predictors documented in this study.

**(KEY FINDING) PM Math Quizzes (r=0.829) predict LEAP Math better than PM Math Tests (r=0.762) or Standards Checkpoints (r=0.746).** This counterintuitive result likely reflects breadth: quizzes are administered more frequently (358 unique quizzes vs 126 tests), generating a more comprehensive aggregate signal across more content areas. The signal emerges from coverage, not rigor.

**(KEY FINDING) PM Math r > 0.80 at ALL four schools.** Ashe 0.832, Green 0.832, LHA 0.819, Wheatley 0.849. This is the most consistent predictor across schools in the entire assessment portfolio. No other predictor achieves r > 0.80 at every school.

**(KEY FINDING) PM Grade 4 Math hits r=0.894 — nearly 80% shared variance.** Grade-level PM Math correlations: G3 0.861, G4 0.894, G5 0.850, G6 0.854, G7 0.855, G8 0.834. All grades above 0.83. Grades 4-5 are the "sweet spot" for PM prediction across all subjects.

**(KEY FINDING) Cross-year validation confirms stable anet-LEAP correlations.** 23-24 anet BOY → 23-24 LEAP: ELA r=0.640 (n=1,741), Math r=0.735 (n=1,709). These replicate the 24-25 values (ELA r=0.633, Math r=0.687) within sampling error. The 23-24 Math correlation is actually HIGHER (0.735 vs 0.687), suggesting the relationship is structural and stable across cohorts.

**(KEY FINDING) ORF is a moderate ELA predictor for grades 3-8 (r=0.563, n=1,596) but substantially weaker than PM or anet.** ORF-LEAP ELA correlations are remarkably stable: BOY r=0.563, MOY r=0.572, EOY r=0.563. ORF also predicts Social Studies (r=0.559) nearly as well as ELA. But PM ELA (r=0.753) and anet ELA EOY (r=0.667) are far superior. ORF adds no incremental value.

**(KEY FINDING) ORF concordance: benchmark status correctly classifies 73.7% of students, with sensitivity of only 55.3%.** ORF misses nearly half of LEAP-proficient students. For comparison, anet BOY <30% catches 81-83% of future Unsatisfactory students. ORF is a weaker screening tool than anet for LEAP risk identification.

**(KEY FINDING) ORF-LEAP correlations replicate perfectly across years.** 23-24 overall r=0.582 vs 24-25 r=0.563 — stable within noise. The relationship is moderate and does not improve with time.

**(CORRECTION) Run 6 reported "zero Social Studies data" in Performance Matters. This was wrong.** PM has 232,670 Social Studies rows across 2,453 students in 2024-25. The Subject value is "Social Stu" (truncated string), not "Social Studies." The Run 6 query searched for the wrong string. PM Social Studies → LEAP Social Studies: r=0.741, making it the ONLY Social Studies predictor available.

**(NULL FINDING) DIBELS 8th Ed Composite does NOT outperform raw ORF alone.** The Composite (ORF + Maze) has a LOWER ELA correlation (r=0.513) than raw ORF (r=0.587). The Maze component adds noise at Grade 8. This applies only to Amplify data (n=319, Grade 8 only).

**(NULL FINDING) 23-24 LEAP Social Studies data is NULL for all records.** SocialScaleScore was NULL across all 4 schools in the 23-24 LEAP data. Social Studies LEAP was either not administered or not loaded for that year. Cross-year SS validation is impossible.

### PM Subject Inventory (2024-25)

| Subject | Rows | Students | Tests | LEAP Match | Match% |
|---|---|---|---|---|---|
| Math | 299,974 | 3,449 | 519 | 1,883 | 54.6% |
| Social Stu | 232,670 | 2,453 | 236 | 1,881 | 76.7% |
| English | 124,316 | 2,493 | 281 | 1,883 | 75.5% |
| Science | 118,933 | 2,448 | 262 | 1,876 | 76.6% |
| Enrichment | 789 | 211 | 18 | — | — |

### PM → LEAP by Assessment Category (Top 10)

| Subject | Category | r | n |
|---|---|---|---|
| Math | Quiz | 0.829 | 1,865 |
| Math | Test | 0.762 | 1,872 |
| Math | Standards Checkpoint | 0.746 | 1,826 |
| English | Test | 0.720 | 1,879 |
| Social Stu | Quiz | 0.704 | 1,876 |
| Science | Quiz | 0.685 | 1,875 |
| English | End of Module | 0.679 | 1,819 |
| Social Stu | Test | 0.667 | 1,854 |
| Science | Test | 0.653 | 1,841 |
| English | Reading Checkpoint | 0.634 | 1,845 |

### Updated Master Predictor Ranking (All Systems, 24-25)

| Rank | System | Predictor | LEAP Subject | r | n | Coverage |
|---|---|---|---|---|---|---|
| 1 | MAP | BOY %ile | Math | 0.840 | 500 | 26% |
| 2 | PM | All tests avg | Math | 0.829 | 1,883 | 99% |
| 3 | PM | All tests avg | ELA | 0.753 | 1,883 | 99% |
| 4 | PM | All tests avg | Soc.Stu | 0.741 | 1,881 | 99% |
| 5 | MAP | BOY %ile | ELA | 0.732 | 1,838 | 93% |
| 6 | anet | MOY % | Math | 0.724 | 1,724 | 88% |
| 7 | PM | All tests avg | Science | 0.714 | 1,876 | 99% |
| 8 | anet | EOY % | Math | 0.689 | 1,843 | 94% |
| 9 | anet | BOY % | Math | 0.687 | 1,787 | 92% |
| 10 | anet | EOY % | ELA | 0.667 | 1,904 | 97% |
| 11 | MAP | BOY RIT | ELA | 0.652 | 1,838 | 93% |
| 12 | anet | BOY % | ELA | 0.633 | 1,840 | 93% |
| 13 | anet | MOY % | ELA | 0.579 | 1,828 | 93% |
| 14 | ORF | MOY Score | ELA | 0.572 | 1,596 | 85% |
| 15 | ORF | BOY Score | ELA | 0.563 | 1,596 | 85% |
| 16 | Grades | Y1 % | ELA | 0.550 | 1,955 | 99% |
| 17 | Amplify | BOY Composite | ELA | 0.513 | 319 | 17% |
| 18 | Grades | Y1 % | Math | 0.411 | 1,937 | 99% |

### ORF Coverage Summary (Grades 3-8, 24-25)

Good coverage for grades 3, 5-8 (~280-625 students). Grade 4 has only 33 students — a near-total gap. AIMSWeb fills this with ~269 Grade 4 students but uses student name identifiers (no LASID), preventing direct LEAP correlation.

### Historical Data Depth

| System | Earliest | Latest | Years |
|---|---|---|---|
| LEAP | 2017-18 | 2024-25 | 7 (missing 19-20) |
| anet | 2023-24 | 2025-26 | 3 |
| MAP | 2024-25 | 2025-26 | 2 |
| PM | 2022-23 | 2024-25 | 3 (22-23 partial) |
| Fluency/ORF | 2020-21 | 2025-26 | 6 |
| AIMSWeb | 2024-25 | 2024-25 | 1 |
| Amplify | 2024-25 | 2024-25 | 1 |
| easyCBM | — | — | Dataset not found |

### Cross-Year Validation Summary

| Pair | 23-24 r | 24-25 r | Δ | Stable? |
|---|---|---|---|---|
| anet ELA BOY → LEAP ELA | 0.640 | 0.633 | +0.007 | Yes |
| anet Math BOY → LEAP Math | 0.735 | 0.687 | +0.048 | Yes (stronger) |
| ORF BOY → LEAP ELA | 0.582 | 0.563 | +0.019 | Yes |

### LEAP Proficiency Trend (3 Years)

| School | 22-23 ELA | 23-24 ELA | 24-25 ELA | 22-23 Math | 23-24 Math | 24-25 Math |
|---|---|---|---|---|---|---|
| Ashe | 33.2% | 37.5% | 36.5% | 18.2% | 23.0% | 20.8% |
| Green | 29.8% | 31.1% | 28.3% | 16.2% | 21.6% | 21.0% |
| LHA | 25.9% | 31.2% | 25.1% | 21.7% | 27.3% | 20.0% |
| Wheatley | 19.7% | 23.8% | 23.3% | 11.1% | 16.3% | 17.2% |

School rankings are highly stable: Ashe leads ELA every year, Wheatley trails every year. The 23-24 spike was network-wide. LHA's decline from 23-24 to 24-25 is the sharpest reversal.

### Schema Gotchas (New)

1. **(CRITICAL) PM Subject "Social Stu" is truncated.** Run 6 searched for "Social Studies" and found nothing. Always use `Subject LIKE 'Social%'` or exact match `'Social Stu'`.
2. **(CRITICAL) PM n values inflate if you don't INNER JOIN to LEAP first.** PM covers K-8; LEAP tests only 3-8. A naive join returns n=3,449 for PM Math including ~1,566 students with no LEAP data. The CORR() function handles NULLs correctly, but the n count is misleading.
3. **(CRITICAL) PM Assessment_Category includes literal `\N` (backslash-N string).** Must filter with raw string syntax: `Assessment_Category != r'\N'` in BigQuery.
4. **(CRITICAL) ORF Grade 4 has only 33 students in orf_data_combined (24-25).** AIMSWeb covers ~269 Grade 4 students but uses student name identifiers (no LASID).
5. **(CRITICAL) 23-24 LEAP SocialScaleScore is NULL for all records.** Cross-year SS validation is impossible.
6. AIMSWeb uses `StudentLastName_StudentFirstName` as identifier, not LASID or Student_Number. Joining to LEAP requires fuzzy name matching.
7. Amplify data covers only Grade 8 (n=319). DIBELS Composite underperforms raw ORF (r=0.513 vs 0.587).
8. ORF School_Year field may be derived from enrollment dates, not test dates. School_Year=2024 and 2023 may overlap.
9. easyCBM dataset not found under any naming variant.
10. DIBELS combined view (`dibels_data_combined_acl`) requires Drive credentials — inaccessible.

### Dead Ends

1. **Run 6 claim of "zero Social Studies data" in PM**: Wrong. Subject is "Social Stu" not "Social Studies." PM SS has 232,670 rows.
2. **DIBELS Composite as superior predictor**: Composite r=0.513 < raw ORF r=0.587. Maze component adds noise for Grade 8.
3. **ORF as competitive LEAP predictor**: r=0.56 overall, far below PM (0.75-0.83) and anet (0.63-0.72). Not worth pursuing.
4. **Cross-year validation for 22-23 anet-LEAP**: No anet data before 23-24. Cannot replicate.
5. **AIMSWeb-LEAP correlation**: Student identifier mismatch prevents reliable joins.
6. **easyCBM data**: Dataset not found in BigQuery.

### Answers to Run 6 Questions

1. **Can PM Science Standards Checkpoints alone predict LEAP Science better than combined PM score?** No. PM Science Standards Checkpoints alone: r=0.575 (n=1,678), weaker than combined PM Science: r=0.714 (n=1,876). Quizzes (r=0.685) are better than Standards Checkpoints, but the full aggregate is best.
2. **Can a combined anet ELA + PM Science composite improve Science prediction?** Not tested in this run, but PM Science (r=0.714) already exceeds anet ELA → LEAP Science (r=0.614 from Run 6). The composite would offer diminishing returns.
3. **What explains the Grade 8 weakness in PM Science → LEAP Science prediction?** Confirmed: r=0.679 for G8 vs r=0.796 for G5. The gap is consistent with increasing curricular divergence at middle school.

### Study Conclusions (All 7 Runs)

1. **PM is the single most valuable assessment system for LEAP prediction.** It covers all 4 subjects, has 99% student coverage, and achieves r=0.71-0.83 across subjects. It was previously overlooked because prior runs focused on anet and MAP.
2. **anet BOY remains the best EARLY-WARNING tool** due to BOY timing (September). PM requires a full year of data to aggregate. For September risk identification, anet BOY <30% remains the recommended threshold.
3. **MAP Math has the highest single-predictor r (0.840) but is impractical at network scale** due to 26% coverage. Expanding MAP Math BOY administration to all schools would add value, but PM already provides r=0.829 with 99% coverage.
4. **Course grades are broken for Math** (r=0.411) and should not be used for LEAP prediction or risk identification. The grade inflation documented in Runs 2 and 4 renders them informationally useless for this purpose.
5. **ORF is a moderate, stable predictor** (r≈0.56) that adds no incremental value beyond PM and anet. It should continue to be used for its intended purpose (fluency monitoring) but not elevated to a LEAP prediction role.
6. **Cross-year validation confirms all key relationships are stable.** The anet-LEAP and ORF-LEAP correlations replicate across the 23-24 and 24-25 cohorts.
7. **Social Studies has been unmonitored until now.** PM Social Studies (r=0.741) is the first documented predictor. This should be operationalized.
8. **Science remains flat at ~12% proficiency.** PM Science (r=0.714) can identify at-risk students, but no intervention signal has emerged from 3 years of data.

---

TODAY'S DATE: 2026-02-23

OUTPUT DIRECTORY: /c/Users/sshirey/bigquery-dashboards/research/assessment-alignment/run-08
LEARNINGS FILE (append only): /c/Users/sshirey/bigquery-dashboards/research/assessment-alignment/learnings.md
DATA DICTIONARY (full reference): /c/Users/sshirey/bigquery-dashboards/data_dictionary.yaml

## Run 8 of 8: Multivariate Prediction Model & Operationalization

Runs 1-7 established the predictive power of individual assessment systems. PM emerged as the strongest single predictor (r=0.71-0.83), anet BOY as the best early-warning signal (r=0.63-0.69 in September), and MAP Math as the highest single-r predictor (r=0.840 but 26% coverage). This run builds the actual multivariate prediction model and produces operationalizable outputs.

### Part A: Data Extract — Build the Master Student Dataset

1. **Extract the master prediction dataset**: Build a single flat file with one row per student who has LEAP 2023-2024 data. For each student, pull:
   - **LEAP outcomes (dependent variables)**: ELAScaleScore, MathScaleScore, ScienceScaleScore, SocialStudiesScaleScore, ELAAchievement, MathAchievement, ScienceAchievement, SocialStudiesAchievement from the 24_25 LEAP tables (399001-399005). CAST scale scores to FLOAT64.
   - **anet BOY (September predictors)**: From ela_boy_24_25 and math_boy_24_25, compute per-student percent correct: SUM(points_received)/SUM(points_possible). Join on sas_id = LASID.
   - **anet MOY (January predictors)**: Same from ela_moy_24_25 and math_moy_24_25.
   - **anet EOY (Spring predictors)**: Same from ela_eoy_24_25 and math_eoy_24_25 if available.
   - **PM aggregate scores**: From `performance_matters.results_raw_2024_2025`, compute per-student average Overall_Test_Score for each Subject (English, Math, Science, 'Social Stu'). Join on State_StudentNumber = LASID. Also compute per-student scores broken out by Assessment_Category (Quiz, Test, Standards Checkpoint, etc.).
   - **MAP scores**: From `map.24_25_boy` and `map.24_25_moy`, pull TestRITScore and TestPercentile by Subject. Join on LASID.
   - **ORF scores**: From `fluency.orf_data_combined` for School_Year=2024 and Grade_Level IN (3,4,5,6,7,8), pull BOY_Reading_Fluency_ORF_Score, MOY_Reading_Fluency_ORF_Score. Join on State_StudentNumber = LASID.
   - **Course grades**: From `grades.current_grades_2024_2025`, compute average grade percentage for ELA and Math courses. Join on Student_Number or LASID.
   - **Demographics**: School, Grade, from LEAP or roster table.
   Save the extract as a JSON file in the output directory. Report the n and coverage rate for each predictor column.

2. **Handle missing data**: Report the missingness pattern. How many students have all predictors? How many are missing MAP (expected: ~74%)? How many are missing ORF? Build a coverage matrix showing which predictor combinations are available for how many students.

### Part B: Multivariate Regression Models

3. **ELA prediction model**: Using Python (scipy, numpy, or sklearn if available — check with `pip list` first), fit multiple linear regression models predicting LEAP ELA Scale Score:
   - Model 1: anet ELA BOY only (baseline — September signal)
   - Model 2: PM ELA only (strongest single predictor)
   - Model 3: anet ELA BOY + PM ELA (does combining help?)
   - Model 4: anet ELA BOY + PM ELA + ORF BOY (does fluency add anything?)
   - Model 5: anet ELA BOY + PM ELA + ORF BOY + Course grade ELA (kitchen sink)
   - Model 6: PM ELA + PM Science + PM Social Studies (cross-subject PM)
   For each model report: R-squared, adjusted R-squared, coefficient for each predictor with p-value, and the incremental R-squared gain from adding each predictor. Use listwise deletion for missing data (analyze only students with all predictors in that model).

4. **Math prediction model**: Same approach predicting LEAP Math Scale Score:
   - Model 1: anet Math BOY only
   - Model 2: PM Math only
   - Model 3: anet Math BOY + PM Math
   - Model 4: anet Math BOY + PM Math + MAP Math BOY %ile (for the ~26% with MAP)
   - Model 5: anet Math BOY + PM Math + Course grade Math
   - Model 6: PM Math + PM Science (cross-subject)

5. **Science prediction model**: Predicting LEAP Science Scale Score:
   - Model 1: PM Science only (r=0.714, best single predictor)
   - Model 2: anet ELA BOY + PM Science (reading → science pathway)
   - Model 3: PM Science + PM ELA + PM Math (all PM subjects)
   - Model 4: PM Science + anet ELA BOY + ORF BOY (all available reading signals)

6. **Social Studies prediction model**: Predicting LEAP Social Studies Scale Score:
   - Model 1: PM Social Studies only (r=0.741)
   - Model 2: anet ELA BOY + PM Social Studies
   - Model 3: PM Social Studies + PM ELA + PM Science (all PM subjects)

7. **Train/test validation**: For the best model per subject, do a proper 70/30 train/test split. Train on 70%, predict on the held-out 30%. Report the test-set R-squared, RMSE, and MAE. Compare test-set performance to training-set performance to check for overfitting. Repeat with 5 random splits and report mean ± SD of test R-squared.

### Part C: Operationalizable Risk Tiers

8. **Calibrate risk tiers from the best model**: Using the best model per subject, compute predicted LEAP scale scores for all students. Map predicted scores to LEAP achievement levels using the known cut scores:
   - Unsatisfactory: below ~725 (verify exact cuts from data)
   - Approaching Basic: ~725-749
   - Basic: ~750-774
   - Mastery: ~775-799
   - Advanced: 800+
   Define risk tiers based on predicted achievement:
   - **On Track**: predicted Mastery or Advanced
   - **Watch**: predicted Basic (could go either way)
   - **Intervention**: predicted Approaching Basic
   - **Crisis**: predicted Unsatisfactory
   Report: n in each tier, actual LEAP proficiency rate per tier, sensitivity (% of actual Unsatisfactory caught by Crisis+Intervention tiers), specificity, false positive rate.

9. **Compare model-based tiers to Run 5 simple tiers**: Run 5 created risk tiers from anet BOY alone. How do the multivariate model tiers compare? Do they catch more at-risk students? Fewer false alarms?

10. **School-specific model performance**: Does the model perform equally well at all 4 schools? Report R-squared and tier accuracy by school. If one school has systematically worse prediction, investigate why.

11. **Grade-specific model performance**: Report R-squared by grade level. Are certain grades harder to predict?

### Part D: Prediction Equations for Operationalization

12. **Write out the final prediction equations**: For each subject, express the best model as a simple formula that could be implemented in a spreadsheet or dashboard:
    - LEAP_ELA_predicted = b0 + b1*(anet_ELA_BOY) + b2*(PM_ELA_avg) + ...
    - Include the actual coefficient values, intercept, and R-squared
    - Include the risk tier cut points

13. **September vs Mid-Year comparison**: What is the R-squared if you can only use September data (anet BOY, MAP BOY, ORF BOY) vs mid-year data (adding PM, MOY scores)? Quantify exactly how much prediction improves from September to January/February.

14. **Identify the diminishing returns boundary**: At what point does adding another predictor gain less than 0.01 R-squared? Document the point of diminishing returns for each subject.

### Output:
- Create a self-contained HTML artifact at `/c/Users/sshirey/bigquery-dashboards/research/assessment-alignment/run-08/prediction-model.html` with:
  - Model comparison tables for each subject (R-squared, coefficients, p-values)
  - Train/test validation results
  - Risk tier distribution and accuracy tables
  - Prediction equations in copy-pasteable format
  - September vs mid-year comparison chart
  - School-level and grade-level model performance
  - Diminishing returns analysis
- Append ALL findings to the learnings file under '## Run 8 - Multivariate Prediction Model'